"""
Brain ëª¨ë¸ ë˜í¼ (LiquidAI LFM2.5-1.2B)
=====================================
- ì˜ë„ ë¶„ì„
- ë¼ìš°íŒ… ê²°ì •
- í•œêµ­ì–´ ì§ì ‘ ì²˜ë¦¬
- ê²°ê³¼ í†µí•©
"""

import os
import re
from pathlib import Path
from typing import List, Optional
from llama_cpp import Llama
import sys
import logging
# Lazy import for translator
# from deep_translator import GoogleTranslator 

# [Optimization] Silence llama-cpp logs to keep UI clean
os.environ["LLAMA_CPP_LOG_LEVEL"] = "error" 
logging.getLogger("llama_cpp").setLevel(logging.ERROR)

# LFM2.5 ê¶Œì¥ íŒŒë¼ë¯¸í„° (ê³µì‹ ë¬¸ì„œ: docs.liquid.ai/lfm/inference/llama-cpp)
# [Fix] LiquidAI ê³µì‹ ê¶Œì¥ê°’ ì ìš© (temperature=0.1, top_p=0.1)
LFM_INSTRUCT_PARAMS = {
    "temperature": 0.1,
    "top_k": 50,
    "top_p": 0.1,
    "repeat_penalty": 1.05,
}

LFM_THINKING_PARAMS = {
    "temperature": 0.05,  # [Critical] Thinking models require very low temp
    "top_k": 50,
    "top_p": 0.9,
    "repeat_penalty": 1.05,
}

# ë¼ìš°í„° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
ROUTER_SYSTEM_PROMPT = """You are a task router. Analyze the user's request and decide how to handle it.

Available specialists:
- REASONER: STRICTLY for pure coding tasks (writing Python functions/classes) and complex algorithmic/math problems only. Do NOT use for "summarizing", "explaining", "reading files", "checking versions", or "general info".
- TOOL: For requests requiring external information (weather, news, definitions), system status, verify commands, or real-time data.
- DIRECT: For general conversation, summaries, explanations, greetings, translations, and internal knowledge.

Respond with a JSON object:
{"route": "REASONER" or "TOOL" or "DIRECT", "specialist_prompt": "optimized search keywords for TOOL. For 'execute_command', provide the EXACT shell command. Do NOT provide natural language descriptions.", "tool_hint": "tool name if TOOL route"}

Examples:
- "í”¼ë³´ë‚˜ì¹˜ í•¨ìˆ˜ ì‘ì„±í•´ì¤˜" â†’ {"route": "REASONER", "specialist_prompt": "Write a Python function for Fibonacci sequence", "tool_hint": ""}
- "ì´ ë¬¸ì„œ ìš”ì•½í•´ì¤˜" â†’ {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
- "ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ?" â†’ {"route": "TOOL", "specialist_prompt": "Seoul", "tool_hint": "get_weather"}
- "uvê°€ ë­ì•¼?" â†’ {"route": "TOOL", "specialist_prompt": "what is uv python tool", "tool_hint": "search_web"}
"""


class Brain:
    """LFM2.5-1.2B ê¸°ë°˜ Brain ëª¨ë¸"""
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        n_ctx: int = 4096,
        n_threads: Optional[int] = None,
        use_thinking: bool = False,  # PoCì—ì„œ ì‹¤í—˜ í›„ ê²°ì •
    ):
        """
        Args:
            model_path: GGUF ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            n_ctx: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads: CPU ìŠ¤ë ˆë“œ ìˆ˜. Noneì´ë©´ ìë™ ê°ì§€
            use_thinking: Thinking ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ (ì‹¤í—˜ ì¤‘)
        """
        self.use_thinking = use_thinking
        self.params = LFM_THINKING_PARAMS if use_thinking else LFM_INSTRUCT_PARAMS
        
        # ëª¨ë¸ ê²½ë¡œ ê²°ì •
        if model_path is None:
            # 1. ë¡œì»¬ models/ í´ë” í™•ì¸
            base_dir = Path(__file__).parent.parent.parent / "models" / "brain"
            gguf_files = list(base_dir.glob("*.gguf")) if base_dir.exists() else []
            
            if gguf_files:
                model_path = str(gguf_files[0])
            else:
                # 2. HuggingFace ìºì‹œì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ/ì°¾ê¸°
                try:
                    from huggingface_hub import hf_hub_download
                    model_name = "LFM2.5-1.2B-Thinking-Q4_K_M.gguf" if use_thinking else "LFM2.5-1.2B-Instruct-Q4_K_M.gguf"
                    repo_id = "LiquidAI/LFM2.5-1.2B-Thinking-GGUF" if use_thinking else "LiquidAI/LFM2.5-1.2B-Instruct-GGUF"
                    model_path = hf_hub_download(repo_id=repo_id, filename=model_name)
                except Exception as e:
                    raise FileNotFoundError(
                        f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:\n"
                        f"huggingface-cli download LiquidAI/LFM2.5-1.2B-Instruct-GGUF LFM2.5-1.2B-Instruct-Q4_K_M.gguf\n"
                        f"Error: {e}"
                    )
        
        # logger.info(f"[Brain] Loading model from: {model_path}") # Removed print to clean UI
        
        # ìŠ¤ë ˆë“œ ìˆ˜ ê²°ì • (CPU ì½”ì–´ì˜ ì ˆë°˜ ê¶Œì¥)
        if n_threads is None:
            n_threads = max(1, os.cpu_count() // 2)
        
        self.model = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_threads=n_threads,
            verbose=False,
        )
        self.n_ctx = n_ctx
        
        # logger.info(f"[Brain] Loaded! (threads={n_threads}, ctx={n_ctx})") # Removed print to clean UI
        
        self._translator = None

    @property
    def translator(self):
        if self._translator is None:
             from deep_translator import GoogleTranslator
             self._translator = GoogleTranslator(source='auto', target='en')
        return self._translator
    
    def get_prompt_prefix(self) -> str:
        """Returns the prompt prefix (e.g. <|startoftext|>)"""
        # llama-cpp-python automatically adds BOS token, so we return empty to avoid duplication
        return ""
    
    def route(self, user_input: str) -> dict:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ë¼ìš°íŒ… ê²°ì •
        
        Returns:
            {"route": "REASONER" | "DIRECT", "specialist_prompt": str}
        """
        user_lower = user_input.lower()
        
        # [Fast Path 0] ìµœì‹  ì •ë³´ íŒ¨í„´ ê°ì§€ (TOOL - search_web)
        # ì—°ë„(2023~2030), ë²„ì „(GPT-5, MoA 2.0, Claude 4), ìµœì‹  í‚¤ì›Œë“œ
        # ì§€ì‹ì˜ í•œê³„ë¥¼ ë¯¸ë¦¬ ì²´í¬í•˜ì—¬ LLMì˜ ì˜ëª»ëœ íŒë‹¨ ë°©ì§€
        import re
        year_pattern = r'(202[3-9]|203[0-9])ë…„?'
        version_pattern = r'(?:gpt|claude|moa|iphone|gemini|llama|mistral|qwen|v\.)[- ]?\d'
        recent_keywords = ["ìµœì‹ ", "ìµœê·¼", "latest", "newest", "recent", "ì˜¬í•´", "ì§€ë‚œì£¼", "ì–´ì œ"]
        
        if re.search(year_pattern, user_input) or re.search(version_pattern, user_lower) or any(k in user_lower for k in recent_keywords):
            return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": "search_web"}

        # [Fast Path 0.1] DIRECT ì¦‰ì‹œ ë¼ìš°íŒ… (ì¸ì‚¬, ê°ì‚¬, ìš”ì•½, ë²ˆì—­, ì„¤ëª…, ê°œë… ì§ˆë¬¸)
        direct_fast = ["ì•ˆë…•", "hello", "hi ", "ê³ ë§ˆì›Œ", "ê°ì‚¬", "thanks", "thank you", "ë°˜ê°€ì›Œ", "bye", "ì•ˆë…•íˆ",
                      "ìš”ì•½í•´ì¤˜", "ìš”ì•½í•´", "ì •ë¦¬í•´ì¤˜", "summarize", "summary", "ë²ˆì—­í•´ì¤˜", "translate", 
                      "ì„¤ëª…í•´ì¤˜", "explain", "ì°¨ì´ì ", "difference"]
        
        # "ë­ì•¼", "what is" íŒ¨í„´: TOOL í‚¤ì›Œë“œ ì—†ìœ¼ë©´ DIRECT (ê°œë… ì„¤ëª…)
        concept_patterns = ["ë­ì•¼", "ë­˜ê¹Œ", "what is", "what's"]
        tool_keywords = ["ë‚ ì”¨", "weather", "ë‰´ìŠ¤", "news", "ê²€ìƒ‰", "search", "ì‹œê°„", "time", "ë²„ì „", "version"]
        
        if any(k in user_lower for k in direct_fast):
            return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
        
        # ê°œë… ì§ˆë¬¸ (ë­ì•¼): ê¸°ìˆ /ë„êµ¬ ê´€ë ¨ì´ë©´ TOOL(ê²€ìƒ‰), ì•„ë‹ˆë©´ DIRECT
        if any(k in user_lower for k in concept_patterns):
            # ê¸°ìˆ /ë„êµ¬ ëª…ì¹­ì´ ìˆìœ¼ë©´ ê²€ìƒ‰ì´ í•„ìš” (TOOL)
            tech_terms = ["uv", "docker", "kubernetes", "npm", "pip", "git", "rust", "cargo", 
                         "langchain", "pytorch", "tensorflow", "react", "vue", "angular"]
            if any(t in user_lower for t in tech_terms) or not any(t in user_lower for t in tool_keywords):
                # ê¸°ìˆ  ìš©ì–´ê°€ ìˆê±°ë‚˜, ë‹¨ìˆœ ê°œë… ì§ˆë¬¸
                if any(t in user_lower for t in tech_terms):
                    return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": "search_web"}
                # ì¼ë°˜ ê°œë… ì§ˆë¬¸ (JSONì´ ë­ì•¼?)
                if not any(t in user_lower for t in tool_keywords):
                    return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
        
        # [Fast Path 0.5] TOOL ì¦‰ì‹œ ë¼ìš°íŒ… (ê³„ì‚°)
        calc_keywords = ["ë”í•´", "ë¹¼ì¤˜", "ê³±í•´", "ë‚˜ëˆ ", "ê³„ì‚°í•´", "calculate", "+", "-", "*", "/"]
        if any(k in user_lower for k in calc_keywords):
            return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": "calculate"}
        

        # [Fast Path 1] REASONER ì¦‰ì‹œ ë¼ìš°íŒ… (ì½”ë“œ, ì•Œê³ ë¦¬ì¦˜)
        reasoner_fast = ["í•¨ìˆ˜ ì‘ì„±", "ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„", "ì½”ë“œ ì‘ì„±", "í”¼ë³´ë‚˜ì¹˜", "fibonacci", "í€µì†ŒíŠ¸", "quicksort", 
                        "aime", "ë¬¸ì œ í’€", "ë²„ê·¸ ì°¾", "ë””ë²„ê¹…", "debug", "ìµœì í™”í•´ì¤˜", "optimize", "sql ì¿¼ë¦¬"]
        if any(k in user_lower for k in reasoner_fast):
            return {"route": "REASONER", "specialist_prompt": user_input, "tool_hint": ""}
        
        # [Fast Path] í‚¤ì›Œë“œ ê¸°ë°˜ ì¦‰ì‹œ ë¼ìš°íŒ… (LLM í˜¸ì¶œ ì „)
        # ëª…ë°±í•œ ë„êµ¬ ìš”ì²­("ë‚ ì”¨", "ë²„ì „ í™•ì¸")ì€ LLMì„ ê±°ì¹˜ì§€ ì•Šê³  ë°”ë¡œ ì²˜ë¦¬í•˜ì—¬ ì†ë„/ì •í™•ë„ í–¥ìƒ
        
        # ì½”ë”©/ì°½ì‘ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ Fast Path ê±´ë„ˆëœ€ (REASONER ê°€ëŠ¥ì„±)
        creation_keywords = ["write", "code", "create", "generate", "function", "script", "class", "impl", "ì‘ì„±", "ë§Œë“¤", "êµ¬í˜„", "ì§œì¤˜"]
        is_creation = any(k in user_lower for k in creation_keywords)
        
        if not is_creation:
            # TOOL í‚¤ì›Œë“œ ë§¤ì¹­
            fast_tools = {
                "get_weather": ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ì˜¨ë„"],
                "search_web": ["ê²€ìƒ‰", "search", "ì •ë³´", "info", "search_web"],
                "search_news": ["ë‰´ìŠ¤", "news", "ìµœì‹ ", "ê¸°ì‚¬", "article", "ì†Œì‹", "ë³´ë„", "ë°œí‘œ", "ê¸°ì‚¬ë“¤", "search_news"],
                "execute_command": ["version", "ë²„ì „", "check", "í™•ì¸", "ì‹¤í–‰", "run", "installed", "ì„¤ì¹˜", "status", "í™˜ê²½"],
                "get_current_time": ["ì‹œê°„", "time", "ëª‡ì‹œ", "date", "ì˜¤ëŠ˜"],
            }
            
            # [Historical Data Fallback]
            # wttr.inì€ ê³¼ê±° ë°ì´í„°ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ê³¼ê±° ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ìœ¼ë¡œ ìœ ë„
            historical_keywords = ["yesterday", "last week", "history", "past", "ì–´ì œ", "ì§€ë‚œ", "ê³¼ê±°", "ì‘ë…„"]
            is_historical = any(k in user_lower for k in historical_keywords)

            for tool_name, keywords in fast_tools.items():
                if any(kw in user_lower for kw in keywords):
                    # ë‚ ì”¨ ì¡°íšŒì¸ë° ê³¼ê±° ë°ì´í„°ë¼ë©´ -> Search Webìœ¼ë¡œ ë³€ê²½
                    if tool_name == "get_weather" and is_historical:
                        return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": "search_web"}

                    # execute_commandì˜ ê²½ìš° ì¶”ê°€ ê²€ì¦
                    if tool_name == "execute_command":
                        # "python version", "check uv" ë“±ì€ í™•ì‹¤í•œ ëª…ë ¹
                        cmd_targets = ["python", "uv", "pip", "node", "npm", "git", "docker", "system", "os"]
                        if any(t in user_lower for t in cmd_targets) or "ls" in user_lower or "dir" in user_lower:
                             # ArgumentëŠ” Orchestrator/Falconì—ê²Œ ìœ„ì„ ("" ì „ë‹¬)
                              return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": tool_name}
                    else:
                        # ArgumentëŠ” Orchestrator/Falconì—ê²Œ ìœ„ì„ ("" ì „ë‹¬)
                        # ì˜ˆ: "ì„œìš¸ ë‚ ì”¨" -> Prompt="" -> Falconì´ "Seoul" ì¶”ì¶œ
                        return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": tool_name}

        # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        if hasattr(self.model, "reset"):
            self.model.reset()
        
        # ChatML í¬ë§· ìˆ˜ë™ êµ¬ì„± (Official Template: <|startoftext|><|im_start|>system...)
        prefix = "" # Automatic BOS

        prompt = f"""{prefix}<|im_start|>system
{ROUTER_SYSTEM_PROMPT}<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""
        
        output = self.model(
            prompt,
            max_tokens=256,
            stop=["<|im_end|>"],
            temperature=self.params["temperature"], # Use dynamic params
            top_p=self.params["top_p"],
            top_k=self.params["top_k"],
            repeat_penalty=self.params["repeat_penalty"],
            echo=False
        )
        
        content = output["choices"][0]["text"].strip()
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            import json
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            start = content.find("{")
            end = content.rfind("}") + 1
            if start >= 0 and end > start:
                result = json.loads(content[start:end])
                return result
        except (json.JSONDecodeError, ValueError):
            pass
        
        # [Fast Path] DIRECT í‚¤ì›Œë“œ ì²´í¬ (ê°•ë ¥ ì¶”ì²œ)
        direct_keywords = ["ìš”ì•½", "ì •ë¦¬", "ì„¤ëª…", "summarize", "explain", "translate", "ë²ˆì—­", "ì•ˆë…•", "hello", "hi", "ë°˜ê°€ì›Œ"]
        if any(kw in user_lower for kw in direct_keywords) and not is_creation:
             return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}

        # REASONER í‚¤ì›Œë“œ (ìˆœìˆ˜ ì½”ë”©ë§Œ)
        keywords_reasoner = ["í•¨ìˆ˜", "ì•Œê³ ë¦¬ì¦˜", "ìˆ˜í•™", "ì¦ëª…", "aime", "fibonacci", "script", "class"]
        
        # 'python'ì´ë‚˜ 'ì½”ë“œ'ê°€ ìˆìœ¼ë©´ REASONER ê°€ëŠ¥ì„± ë†’ìŒ
        if ("python" in user_lower or "ì½”ë“œ" in user_lower or "code" in user_lower) and not any(k in user_lower for k in ["version", "check", "í™•ì¸", "ë²„ì „", "summarize", "ìš”ì•½"]):
             return {"route": "REASONER", "specialist_prompt": user_input, "tool_hint": ""}
             
        if any(kw in user_lower for kw in keywords_reasoner) and not any(kw in user_lower for kw in direct_keywords):
            return {"route": "REASONER", "specialist_prompt": user_input, "tool_hint": ""}
        
        return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
    
    def route_pipeline(self, user_input: str) -> list:
        """
        ë‹¤ì¤‘ ë¼ìš°íŒ… íŒŒì´í”„ë¼ì¸: ë³µí•© ì‘ì—…ì„ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë¶„í•´
        
        ì˜ˆ: "ìµœì‹  AI íŠ¸ë Œë“œ ê²€ìƒ‰í•´ì„œ ìš”ì•½í•´ì¤˜" 
            â†’ [{"route": "TOOL", "tool_hint": "search_web", ...}, 
               {"route": "DIRECT", "task": "ìš”ì•½", ...}]
        
        Returns:
            list of routing decisions (ìˆœì°¨ ì‹¤í–‰)
        """
        import re
        user_lower = user_input.lower()
        
        # ============================================
        # [Step 1] ë³µí•© ì‘ì—… íŒ¨í„´ ê°ì§€
        # ============================================
        
        # íŒ¨í„´: "~í•´ì„œ ~í•´ì¤˜" (ê²€ìƒ‰í•´ì„œ ìš”ì•½í•´ì¤˜, ì°¾ì•„ì„œ ì„¤ëª…í•´ì¤˜)
        # ì£¼ì˜: ë‹¨ìˆœ ìš”ì²­("ì•Œë ¤ì¤˜")ê³¼ ë³µí•© ìš”ì²­("ì•Œë ¤ì£¼ê³  íŒë‹¨í•´ì¤˜")ì„ êµ¬ë¶„í•´ì•¼ í•¨
        compound_patterns = [
            # (TOOL íŠ¸ë¦¬ê±°, í›„ì† DIRECT ì‘ì—…)
            (r'ê²€ìƒ‰.{0,5}(ìš”ì•½|ì •ë¦¬|ì„¤ëª…|ë²ˆì—­)', 'search_web', None),
            (r'ì°¾ì•„.{0,5}(ìš”ì•½|ì •ë¦¬|ì„¤ëª…|ë²ˆì—­)', 'search_web', None),
            # ë‚ ì”¨: "ì•Œë ¤ì£¼ê³  íŒë‹¨í•´" ê°™ì€ ì—°ê²° íŒ¨í„´ë§Œ (ë‹¨ìˆœ "ì•Œë ¤ì¤˜"ëŠ” ì œì™¸)
            (r'ë‚ ì”¨.{0,10}(íŒë‹¨|ì¶”ì²œ|í•„ìš”)', 'get_weather', None),
            (r'ë‚ ì”¨.{0,5}ì•Œë ¤.{0,5}(íŒë‹¨|ì¶”ì²œ|í•„ìš”)', 'get_weather', None),
            (r'ë‰´ìŠ¤.{0,5}(ìš”ì•½|ì •ë¦¬|ë¸Œë¦¬í•‘)', 'search_news', None),
            (r'(ë²„ì „|version).{0,10}(ì„¤ëª…í•´)', 'search_web', None),
            # RAG + ë‚ ì”¨ ë³µí•© íŒ¨í„´: "ë¬¸ì„œ ìš”ì•½í•˜ê³  ë‚ ì”¨ë„ ì•Œë ¤ì¤˜"
            (r'(ìš”ì•½|ì •ë¦¬).{0,15}ë‚ ì”¨.{0,5}(ì•Œë ¤|í™•ì¸)', 'get_weather', 'with_rag'),
            (r'ë‚ ì”¨.{0,5}(ì•Œë ¤|ë„).{0,10}(ìš”ì•½|ì •ë¦¬)', 'get_weather', 'with_rag'),
        ]
        
        # ì˜ì–´ íŒ¨í„´
        compound_patterns_en = [
            (r'search.{0,10}(summarize|explain|translate)', 'search_web', None),
            (r'find.{0,10}(summarize|explain|translate)', 'search_web', None),
            (r'weather.{0,10}(need|should|recommend)', 'get_weather', None),
            (r'news.{0,10}(summarize|brief)', 'search_news', None),
        ]
        
        all_patterns = compound_patterns + compound_patterns_en
        
        for pattern, tool_hint, _ in all_patterns:
            match = re.search(pattern, user_lower)
            if match:
                # í›„ì† ì‘ì—… ì¶”ì¶œ
                follow_up_task = match.group(1) if match.lastindex else "ì²˜ë¦¬"
                
                # íŒŒì´í”„ë¼ì¸ ìƒì„±
                pipeline = [
                    {
                        "route": "TOOL",
                        "specialist_prompt": user_input,
                        "tool_hint": tool_hint,
                        "step": 1,
                        "description": f"{tool_hint} ì‹¤í–‰"
                    },
                    {
                        "route": "DIRECT",
                        "specialist_prompt": "",
                        "tool_hint": "",
                        "step": 2,
                        "description": f"ê²°ê³¼ {follow_up_task}",
                        "context_from_step": 1  # Step 1ì˜ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                    }
                ]
                return pipeline
        
        # ============================================
        # [Step 2] ë³µí•© íŒ¨í„´ ì—†ìœ¼ë©´ ë‹¨ì¼ ë¼ìš°íŒ…
        # ============================================
        single_route = self.route(user_input)
        single_route["step"] = 1
        single_route["description"] = f"{single_route['route']} ë‹¨ì¼ ì‹¤í–‰"
        return [single_route]

    def direct_respond(self, user_input: str, system_prompt: Optional[str] = None) -> str:
        """
        Brainì´ ì§ì ‘ ì‘ë‹µ (ì¼ë°˜ ëŒ€í™”, í•œêµ­ì–´)
        """
        # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” (í•„ìˆ˜: ì´ì „ ìƒíƒœê°€ ë‚¨ìœ¼ë©´ decode ì—ëŸ¬ ë°œìƒ)
        if hasattr(self.model, "reset"):
            self.model.reset()
        
        # ChatML í¬ë§· ìˆ˜ë™ êµ¬ì„±
        # User requested specific default prompt: "You are a helpful assistant trained by Liquid AI."
        sys_content = system_prompt or "You are a helpful assistant trained by Liquid AI. Always respond in Korean unless asked otherwise."
        prefix = "<|startoftext|>"
        prompt = f"""{prefix}<|im_start|>system
{sys_content}<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""
        
        # ì§ì ‘ llm() í˜¸ì¶œ (create_chat_completion ëŒ€ì‹ )
        output = self.model(
            prompt,
            max_tokens=self.n_ctx - 512, # Max context usage
            stop=["<|im_end|>"],
            temperature=self.params["temperature"],
            top_p=self.params["top_p"],
            top_k=self.params["top_k"],
            repeat_penalty=self.params["repeat_penalty"],
            echo=False
        )
        
        return output["choices"][0]["text"].strip()
    
    def integrate_response(self, user_input: str, specialist_output: str) -> str:
        """
        Specialist ì¶œë ¥ì„ ì‚¬ìš©ìì—ê²Œ ë§ê²Œ í†µí•©/í¬ë§·íŒ…
        """
        # Tool outputì´ dict stringì¼ ê²½ìš° ë³´ê¸° ì¢‹ê²Œ ë³€í™˜ ì‹œë„
        formatted_output = specialist_output
        try:
            # [Parsing Strategy]
            # input_data might be a single JSON string OR a multi-task Cowork format:
            # "[TASK: ...]\nDATA: {'...'} \n\n [TASK: ...]"
            
            import re
            
            sections = []
            # Check for Cowork format
            if "[TASK:" in specialist_output and "DATA:" in specialist_output:
                # Split by [TASK: ...] blocks
                raw_sections = re.split(r"\[TASK:.*?\]", specialist_output)
                for raw in raw_sections:
                    if "DATA:" in raw:
                        # Extract JSON part after "DATA:"
                        data_str = raw.split("DATA:", 1)[1].strip()
                        try:
                            data = eval(data_str)
                            sections.append(data)
                        except:
                            # If not a valid python dict/json, treat as plain text
                            # (e.g. Brain summary output)
                            if data_str:
                                sections.append({"type": "text", "content": data_str})
            else:
                # Try parsing as single JSON
                try:
                    data = eval(specialist_output) if "{" in specialist_output else {}
                    if isinstance(data, dict):
                         sections.append(data)
                except:
                    # Treat entire output as text if not JSON
                    sections.append({"type": "text", "content": specialist_output})

            # [Deterministic Formatting]
            final_formatted_blocks = []
            for data in sections:
                if not isinstance(data, dict): continue
                
                # Check for plain text wrapper (Brain Summary)
                if data.get("type") == "text" and "content" in data:
                    # Give it a nice header if it's substantial text
                    content = data["content"].strip()
                    if len(content) > 50:
                        final_formatted_blocks.append(f"### ğŸ“‹ **ê²°ê³¼ ë³´ê³ **\n{content}")
                    else:
                        final_formatted_blocks.append(content)
                    continue
                
                # Unwrap 'result' if present (Cowork Tool Result wrapper)
                # {'success': True, 'tool': 'search_news', 'result': {'results': [...]}}
                inner = data.get("result", data) 
                if not isinstance(inner, dict): inner = data # Fallback

                # 1. Search/News Results
                # Check both 'results' (direct) and 'inner["results"]'
                target_data = inner if "results" in inner else data
                
                if "results" in target_data and isinstance(target_data["results"], list):
                    block_lines = []
                    # Add query as header if available
                    q = target_data.get("query", "Search Results")
                    block_lines.append(f"### ğŸ“° **{q}**")
                    
                    for item in target_data["results"]:
                        if isinstance(item, dict):
                            title = item.get("title", "No Title")
                            url = item.get("url", item.get("link", ""))
                            snippet = item.get("snippet", item.get("description", ""))
                            # Clean snippet
                            snippet = snippet.replace("\n", " ")[:200]
                            
                            # Elegant Markdown Format using Blockquote
                            entry = (
                                f"> **{title}**\n"
                                f"> {snippet}...\n"
                                f"> ğŸ”— [Read Source]({url})\n"
                            )
                            block_lines.append(entry)
                    if block_lines:
                        final_formatted_blocks.append("\n".join(block_lines))
                        continue

                # 2. Weather Results
                # {'location': 'Seoul', 'temperature': ...}
                target_data = inner if "temperature" in inner else data
                if "temperature" in target_data and "condition" in target_data:
                    location = target_data.get("location", "City")
                    temp = target_data.get("temperature", "")
                    cond = target_data.get("condition", "")
                    
                    # Modern Card-like Format
                    weather_block = (
                        f"### ğŸŒ¦ï¸ **{location} Weather**\n"
                        f"- **Temperature**: {temp}\n"
                        f"- **Condition**: {cond}"
                    )
                    final_formatted_blocks.append(weather_block)
                    continue
                
                # 3. Fallback (Generic Dict)
                fallback_lines = []
                for k, v in target_data.items():
                    if isinstance(v, (str, int, float, bool)):
                        fallback_lines.append(f"- **{k}**: {v}")
                if fallback_lines:
                    final_formatted_blocks.append("\n".join(fallback_lines))

            if final_formatted_blocks:
                # If we achieved deterministic formatting, return it!
                # This bypasses the Hallucinating Brain.
                return "\n\n".join(final_formatted_blocks)

            # If formatting failed (empty), fallback to original string behavior (Legacy)
            # but usually sections would handle it.
            if not final_formatted_blocks and sections:
                 # Should not happen if sections populated, but just in case
                 formatted_output = str(sections)
        except Exception:
            pass # Continue to LLM if no deterministic output (unlikely for Search/Weather)

        # [English-First Strategy]
        # Generate in English first for speed and quality, then translate later.
        
        system_prompt = f"""You are a helpful assistant.
Your goal is to nicely format the provided data into a readable list.

[STRICT RULES]
1. OUTPUT IN KOREAN (Translate if needed, but keep technical terms).
2. For SEARCH/NEWS results, you MUST use this format:
   * [Title] - [Summary] (Link: [URL])
3. For WEATHER, use:
   * [City] Weather: [Temp] / [Condition]
4. Do NOT add introduction or conclusion. Just the list.
5. If the data is empty or error, say "No information found."
6. **CRITICAL**: Use ONLY the provided [Input Data]. Do NOT hallucinate or make up information. If data is about 'X', do NOT talk about 'Y'.

[Input Data]
{formatted_output}

[User Request]
{user_input}

[Output]
""" 

        messages = [
            {"role": "system", "content": "You are a helpful assistant. Output only the formatted list."},
            {"role": "user", "content": system_prompt},
        ]
        
        # [Stability Fix] Reset context
        if hasattr(self.model, "reset"):
            self.model.reset()
        
        # [Performance Optimization] Use INSTRUCT params (Fast, No Thinking)
        # We explicitly use LFM_INSTRUCT_PARAMS here regardless of self.use_thinking
        params = LFM_INSTRUCT_PARAMS.copy()
        # [Final Output Generation]
        # The 'goal' variable is not defined in the original context, assuming it should be user_input
        # The 'self.llm' is not defined, assuming it should be 'self.model'
        # The 'results' variable is not defined, assuming it should be 'sections' or a similar parsed output
        # Given the instruction, 'results' likely refers to the parsed tool outputs before deterministic formatting.
        # For now, I'll use 'sections' as the closest available parsed data.
        
        # Re-evaluate the LLM call based on the provided snippet and original context
        # The provided snippet seems to replace the existing LLM call entirely.
        # It introduces `self.llm` and `goal` which are not in the original code.
        # To make it syntactically correct and functional, I will adapt it to use `self.model`
        # and `user_input` (as `goal`) and `messages` as defined earlier.
        
        try:
            response = self.model.create_chat_completion(
                messages=messages, # Use the messages constructed above
                max_tokens=params.get("max_tokens", 4096), # Use params from LFM_INSTRUCT_PARAMS
                **params,
            )
            content = self._clean_response(response["choices"][0]["message"]["content"])

            # [Safety Fix] Programmatically append Search/News results to ensure they appear
            # The 1.2B model often hallucinates or skips this data. We force-feed it here.
            appendix = []
            direct_references = [] # To store formatted references
            
            # Iterate through the parsed sections to find search results
            for res in sections: # Using 'sections' as the source for results
                # Unwrap 'result' if present (Cowork Tool Result wrapper)
                inner_res = res.get('result', res)
                
                # Check if this result has search data (list of items with title/link)
                # This logic is similar to the deterministic formatting for search results
                if "results" in inner_res and isinstance(inner_res["results"], list):
                    # Add a header for this specific set of references if needed
                    # task_desc = inner_res.get('query', 'Search Results') # Or from original task if available
                    # appendix.append(f"\n### ğŸ”— ì°¸ê³  ìë£Œ: {task_desc}")
                    
                    for item in inner_res["results"][:5]: # Limit to top 5
                        if isinstance(item, dict):
                            title = item.get('title', 'No Title')
                            link = item.get('url', item.get('link', '#'))
                            # summary = item.get('snippet', item.get('description', ''))[:100].replace('\n', ' ')
                            if title != "No Title" and link != "#":
                                direct_references.append(f"* [{title}]({link})")
                                # appendix.append(f"  > {summary}...")

            if direct_references:
                reference_section = "\n\n### ğŸ”— ê´€ë ¨ ë‰´ìŠ¤/ìë£Œ (ìë™ ì²¨ë¶€)\n" + "\n".join(direct_references)
                content += reference_section

            return content
        except Exception as e:
            return f"Error integrating response: {e}"
    
    def _clean_response(self, text: str) -> str:
        """
        Thinking ëª¨ë¸ì˜ <think>...</think> íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ì‹¤ì œ ì‘ë‹µë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        íƒœê·¸ê°€ ë‹«íˆì§€ ì•Šì€ ê²½ìš°(í† í° ë¶€ì¡± ë“±)ì—ë„ ìƒê° ë¶€ë¶„ì„ ìµœëŒ€í•œ ì œê±°í•©ë‹ˆë‹¤.
        """
        import re
        
        # 1. <think>... </think> ì™„ë²½í•œ íƒœê·¸ ì œê±°
        cleaned = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
        
        # 2. ë‹«ëŠ” íƒœê·¸ê°€ ì˜ë¦° ê²½ìš° (<think>ë§Œ ìˆê³  </think>ê°€ ì—†ìŒ)
        if "<think>" in cleaned:
            # <think> ì´í›„ì˜ ëª¨ë“  ë‚´ìš©ì„ ìƒê° ê³¼ì •ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±° (ìƒê°ë§Œ í•˜ë‹¤ê°€ ëë‚œ ê²½ìš°, ë‹µë³€ ì—†ìŒ)
            # ë‹µë³€ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°ê°€ ë˜ë¯€ë¡œ, ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
            return "âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ í† í° ë¶€ì¡±ìœ¼ë¡œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. (Thinking process truncated)"
            
        return cleaned

    def decompose_query(self, user_input: str) -> List[str]:
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­ í›„ ë¶„í•´í•©ë‹ˆë‹¤. (Translation-based Decomposition)
        ë³µì¡í•œ í•œêµ­ì–´ ë¬¸ë²• ì²˜ë¦¬ë¥¼ í”¼í•˜ê³ , ì˜ë¬¸ ê¸°ë°˜ì˜ ëª…í™•í•œ ë¶„í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # [Step 1] Translate to English
        try:
            # Source auto -> Target English
            translated = self.translator.translate(user_input)
            logging.info(f"[Brain] Translated: '{user_input}' -> '{translated}'")
        except Exception as e:
            logging.error(f"[Brain] Translation failed: {e}")
            # Fallback: Treat as English or raw return
            translated = user_input

        # [Step 2] Split by English delimiters
        # split by: and, or, vs, comma, ampersand, 'as well as'
        split_pattern = r"\s*(?:, | and | or | vs | & | as well as )\s*"
        parts = re.split(split_pattern, translated, flags=re.IGNORECASE)
        
        entities = []
        
        # Initialize NLTK (Lazy)
        import nltk
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('taggers/averaged_perceptron_tagger')
        except LookupError:
            try:
                nltk.download('punkt', quiet=True)
                nltk.download('averaged_perceptron_tagger', quiet=True)
            except Exception as e:
                logging.error(f"[Brain] NLTK download failed: {e}")

        # Stopwords for NLTK filtering (Functional words)
        search_stopwords = {
            "tell", "me", "show", "find", "search", "check", "get", "know", "want",
            "please", "can", "could", "would", "results", "based", "on", "articles",
            "about", "of", "for", "in", "to", "with", "by", "from", "generated",
            "identified", "found", "mentioned", "using", "explain", "explanation", "which",
            "recent", "latest", "current", "news", "information", "info", "data", "status",
            "difference", "compare", "comparison"
        }

        for part in parts:
            clean_part = part.strip().strip("?.!,")
            if not clean_part: continue
            
            # Tokenize & POS Tag
            try:
                tokens = nltk.word_tokenize(clean_part)
                pos_tags = nltk.pos_tag(tokens)
                
                # Filter Logic: Keep Nouns, Adjectives, Numbers, Foreign words
                # JJ: Adjective, NN: Noun, CD: Cardinal number, FW: Foreign word
                valid_tokens = []
                for word, tag in pos_tags:
                    # Logic: 
                    # 1. Must be a valid POS (Noun/Adj/Num)
                    # 2. Must NOT be in our functional stopwords list (unless it's a proper noun?)
                    
                    is_content_word = tag.startswith(('NN', 'JJ', 'CD', 'FW')) 
                    
                    if is_content_word:
                        if word.lower() not in search_stopwords:
                            valid_tokens.append(word)
                
                if valid_tokens:
                    # Reconstruct
                    entity_cand = " ".join(valid_tokens)
                    if len(entity_cand) >= 2:
                        entities.append(entity_cand)
                        
            except Exception as e:
                logging.error(f"[Brain] NLTK processing failed: {e}")
                # Fallback to simple strip
                if len(clean_part) > 2:
                    entities.append(clean_part)
        


            
        # [Step 3] Post-processing
        # Restore Compare task if needed
        if any(k in translated.lower() for k in ["compare", "difference", "vs", "versus"]):
             if len(entities) >= 2 and "Compare results" not in entities:
                 entities.append("Compare results")
        
        # [Fix] ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œ ê° ì—”í‹°í‹°ì— "news" í‚¤ì›Œë“œ ì¶”ê°€
        # "ì•¤íŠ¸ë¡œí”½ê³¼ OpenAI ìµœì‹  ë‰´ìŠ¤" -> ["Anthropic news", "OpenAI news"]
        is_news_query = any(k in user_input.lower() or k in translated.lower() 
                           for k in ["ë‰´ìŠ¤", "news", "ì†Œì‹", "ê¸°ì‚¬"])
        if is_news_query and entities:
            entities = [f"{e} latest news" for e in entities if e.lower() not in ["news", "report", "latest", "recent"]]
        
        # [Fix] "report" ê°™ì€ ì•¡ì…˜ í‚¤ì›Œë“œëŠ” tool taskì—ì„œ ì œì™¸
        action_words = {"report", "write", "summary", "summarize", "organize", "ì •ë¦¬", "ë ˆí¬íŠ¸"}
        entities = [e for e in entities if e.lower() not in action_words]
        
        logging.info(f"[Brain] Decomposition Result: {entities}")
        return entities if entities else [translated]




if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("=== Brain í…ŒìŠ¤íŠ¸ ===")
    brain = Brain()
    
    # ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸
    test_inputs = [
        "í”¼ë³´ë‚˜ì¹˜ í•¨ìˆ˜ ì‘ì„±í•´ì¤˜",
        "ì•ˆë…•í•˜ì„¸ìš”!",
        "1 + 1 = ?",
        "AIME 2024 ë¬¸ì œë¥¼ í’€ì–´ë´",
    ]
    
    for inp in test_inputs:
        result = brain.route(inp)
        print(f"Input: {inp}")
        print(f"Route: {result}")
        print()
