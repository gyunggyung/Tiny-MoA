"""
Brain ëª¨ë¸ ë˜í¼ (LiquidAI LFM2.5-1.2B)
=====================================
- ì˜ë„ ë¶„ì„
- ë¼ìš°íŒ… ê²°ì •
- í•œêµ­ì–´ ì§ì ‘ ì²˜ë¦¬
- ê²°ê³¼ í†µí•©
"""

import os
import re
from pathlib import Path
from typing import List, Optional
from llama_cpp import Llama
import sys
import logging

# [Optimization] Silence llama-cpp logs to keep UI clean
os.environ["LLAMA_CPP_LOG_LEVEL"] = "error" 
logging.getLogger("llama_cpp").setLevel(logging.ERROR)

# LFM2.5 ê¶Œì¥ íŒŒë¼ë¯¸í„° (ê³µì‹ ë¬¸ì„œ: docs.liquid.ai/lfm/inference/llama-cpp)
LFM_INSTRUCT_PARAMS = {
    "temperature": 0.7,
    "top_k": 40,
    "top_p": 0.9,
    "repeat_penalty": 1.1,
}

LFM_THINKING_PARAMS = {
    "temperature": 0.05,  # [Critical] Thinking models require very low temp
    "top_k": 50,
    "top_p": 0.9,
    "repeat_penalty": 1.05,
}

# ë¼ìš°í„° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
ROUTER_SYSTEM_PROMPT = """You are a task router. Analyze the user's request and decide how to handle it.

Available specialists:
- REASONER: STRICTLY for pure coding tasks (writing Python functions/classes) and complex algorithmic/math problems only. Do NOT use for "summarizing", "explaining", "reading files", "checking versions", or "general info".
- TOOL: For requests requiring external information (weather, news, definitions), system status, verify commands, or real-time data.
- DIRECT: For general conversation, summaries, explanations, greetings, translations, and internal knowledge.

Respond with a JSON object:
{"route": "REASONER" or "TOOL" or "DIRECT", "specialist_prompt": "optimized search keywords for TOOL. For 'execute_command', provide the EXACT shell command. Do NOT provide natural language descriptions.", "tool_hint": "tool name if TOOL route"}

Examples:
- "í”¼ë³´ë‚˜ì¹˜ í•¨ìˆ˜ ì‘ì„±í•´ì¤˜" â†’ {"route": "REASONER", "specialist_prompt": "Write a Python function for Fibonacci sequence", "tool_hint": ""}
- "ì´ ë¬¸ì„œ ìš”ì•½í•´ì¤˜" â†’ {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
- "ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ?" â†’ {"route": "TOOL", "specialist_prompt": "Seoul", "tool_hint": "get_weather"}
- "uvê°€ ë­ì•¼?" â†’ {"route": "TOOL", "specialist_prompt": "what is uv python tool", "tool_hint": "search_web"}
"""


class Brain:
    """LFM2.5-1.2B ê¸°ë°˜ Brain ëª¨ë¸"""
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        n_ctx: int = 4096,
        n_threads: Optional[int] = None,
        use_thinking: bool = False,  # PoCì—ì„œ ì‹¤í—˜ í›„ ê²°ì •
    ):
        """
        Args:
            model_path: GGUF ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            n_ctx: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads: CPU ìŠ¤ë ˆë“œ ìˆ˜. Noneì´ë©´ ìë™ ê°ì§€
            use_thinking: Thinking ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ (ì‹¤í—˜ ì¤‘)
        """
        self.use_thinking = use_thinking
        self.params = LFM_THINKING_PARAMS if use_thinking else LFM_INSTRUCT_PARAMS
        
        # ëª¨ë¸ ê²½ë¡œ ê²°ì •
        if model_path is None:
            # 1. ë¡œì»¬ models/ í´ë” í™•ì¸
            base_dir = Path(__file__).parent.parent.parent / "models" / "brain"
            gguf_files = list(base_dir.glob("*.gguf")) if base_dir.exists() else []
            
            if gguf_files:
                model_path = str(gguf_files[0])
            else:
                # 2. HuggingFace ìºì‹œì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ/ì°¾ê¸°
                try:
                    from huggingface_hub import hf_hub_download
                    model_name = "LFM2.5-1.2B-Thinking-Q4_K_M.gguf" if use_thinking else "LFM2.5-1.2B-Instruct-Q4_K_M.gguf"
                    repo_id = "LiquidAI/LFM2.5-1.2B-Thinking-GGUF" if use_thinking else "LiquidAI/LFM2.5-1.2B-Instruct-GGUF"
                    model_path = hf_hub_download(repo_id=repo_id, filename=model_name)
                except Exception as e:
                    raise FileNotFoundError(
                        f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:\n"
                        f"huggingface-cli download LiquidAI/LFM2.5-1.2B-Instruct-GGUF LFM2.5-1.2B-Instruct-Q4_K_M.gguf\n"
                        f"Error: {e}"
                    )
        
        # logger.info(f"[Brain] Loading model from: {model_path}") # Removed print to clean UI
        
        # ìŠ¤ë ˆë“œ ìˆ˜ ê²°ì • (CPU ì½”ì–´ì˜ ì ˆë°˜ ê¶Œì¥)
        if n_threads is None:
            n_threads = max(1, os.cpu_count() // 2)
        
        self.model = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_threads=n_threads,
            verbose=False,
        )
        self.n_ctx = n_ctx
        
        # logger.info(f"[Brain] Loaded! (threads={n_threads}, ctx={n_ctx})") # Removed print to clean UI
    
    def get_prompt_prefix(self) -> str:
        """Returns the prompt prefix (e.g. <|startoftext|>)"""
        return "<|startoftext|>"
    
    def route(self, user_input: str) -> dict:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ë¼ìš°íŒ… ê²°ì •
        
        Returns:
            {"route": "REASONER" | "DIRECT", "specialist_prompt": str}
        """
        user_lower = user_input.lower()
        
        # [Fast Path 0] ìµœì‹  ì •ë³´ íŒ¨í„´ ê°ì§€ (TOOL - search_web)
        # ì—°ë„(2023~2030), ë²„ì „(GPT-5, MoA 2.0, Claude 4), ìµœì‹  í‚¤ì›Œë“œ
        # ì§€ì‹ì˜ í•œê³„ë¥¼ ë¯¸ë¦¬ ì²´í¬í•˜ì—¬ LLMì˜ ì˜ëª»ëœ íŒë‹¨ ë°©ì§€
        import re
        year_pattern = r'(202[3-9]|203[0-9])ë…„?'
        version_pattern = r'(?:gpt|claude|moa|iphone|gemini|llama|mistral|qwen|v\.)[- ]?\d'
        recent_keywords = ["ìµœì‹ ", "ìµœê·¼", "latest", "newest", "recent", "ì˜¬í•´", "ì§€ë‚œì£¼", "ì–´ì œ"]
        
        if re.search(year_pattern, user_input) or re.search(version_pattern, user_lower) or any(k in user_lower for k in recent_keywords):
            return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": "search_web"}

        # [Fast Path 0.1] DIRECT ì¦‰ì‹œ ë¼ìš°íŒ… (ì¸ì‚¬, ê°ì‚¬, ìš”ì•½, ë²ˆì—­, ì„¤ëª…, ê°œë… ì§ˆë¬¸)
        direct_fast = ["ì•ˆë…•", "hello", "hi ", "ê³ ë§ˆì›Œ", "ê°ì‚¬", "thanks", "thank you", "ë°˜ê°€ì›Œ", "bye", "ì•ˆë…•íˆ",
                      "ìš”ì•½í•´ì¤˜", "ìš”ì•½í•´", "ì •ë¦¬í•´ì¤˜", "summarize", "summary", "ë²ˆì—­í•´ì¤˜", "translate", 
                      "ì„¤ëª…í•´ì¤˜", "explain", "ì°¨ì´ì ", "difference"]
        
        # "ë­ì•¼", "what is" íŒ¨í„´: TOOL í‚¤ì›Œë“œ ì—†ìœ¼ë©´ DIRECT (ê°œë… ì„¤ëª…)
        concept_patterns = ["ë­ì•¼", "ë­˜ê¹Œ", "what is", "what's"]
        tool_keywords = ["ë‚ ì”¨", "weather", "ë‰´ìŠ¤", "news", "ê²€ìƒ‰", "search", "ì‹œê°„", "time", "ë²„ì „", "version"]
        
        if any(k in user_lower for k in direct_fast):
            return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
        
        # ê°œë… ì§ˆë¬¸ (ë­ì•¼): ê¸°ìˆ /ë„êµ¬ ê´€ë ¨ì´ë©´ TOOL(ê²€ìƒ‰), ì•„ë‹ˆë©´ DIRECT
        if any(k in user_lower for k in concept_patterns):
            # ê¸°ìˆ /ë„êµ¬ ëª…ì¹­ì´ ìˆìœ¼ë©´ ê²€ìƒ‰ì´ í•„ìš” (TOOL)
            tech_terms = ["uv", "docker", "kubernetes", "npm", "pip", "git", "rust", "cargo", 
                         "langchain", "pytorch", "tensorflow", "react", "vue", "angular"]
            if any(t in user_lower for t in tech_terms) or not any(t in user_lower for t in tool_keywords):
                # ê¸°ìˆ  ìš©ì–´ê°€ ìˆê±°ë‚˜, ë‹¨ìˆœ ê°œë… ì§ˆë¬¸
                if any(t in user_lower for t in tech_terms):
                    return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": "search_web"}
                # ì¼ë°˜ ê°œë… ì§ˆë¬¸ (JSONì´ ë­ì•¼?)
                if not any(t in user_lower for t in tool_keywords):
                    return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
        
        # [Fast Path 0.5] TOOL ì¦‰ì‹œ ë¼ìš°íŒ… (ê³„ì‚°)
        calc_keywords = ["ë”í•´", "ë¹¼ì¤˜", "ê³±í•´", "ë‚˜ëˆ ", "ê³„ì‚°í•´", "calculate", "+", "-", "*", "/"]
        if any(k in user_lower for k in calc_keywords):
            return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": "calculate"}
        

        # [Fast Path 1] REASONER ì¦‰ì‹œ ë¼ìš°íŒ… (ì½”ë“œ, ì•Œê³ ë¦¬ì¦˜)
        reasoner_fast = ["í•¨ìˆ˜ ì‘ì„±", "ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„", "ì½”ë“œ ì‘ì„±", "í”¼ë³´ë‚˜ì¹˜", "fibonacci", "í€µì†ŒíŠ¸", "quicksort", 
                        "aime", "ë¬¸ì œ í’€", "ë²„ê·¸ ì°¾", "ë””ë²„ê¹…", "debug", "ìµœì í™”í•´ì¤˜", "optimize", "sql ì¿¼ë¦¬"]
        if any(k in user_lower for k in reasoner_fast):
            return {"route": "REASONER", "specialist_prompt": user_input, "tool_hint": ""}
        
        # [Fast Path] í‚¤ì›Œë“œ ê¸°ë°˜ ì¦‰ì‹œ ë¼ìš°íŒ… (LLM í˜¸ì¶œ ì „)
        # ëª…ë°±í•œ ë„êµ¬ ìš”ì²­("ë‚ ì”¨", "ë²„ì „ í™•ì¸")ì€ LLMì„ ê±°ì¹˜ì§€ ì•Šê³  ë°”ë¡œ ì²˜ë¦¬í•˜ì—¬ ì†ë„/ì •í™•ë„ í–¥ìƒ
        
        # ì½”ë”©/ì°½ì‘ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ Fast Path ê±´ë„ˆëœ€ (REASONER ê°€ëŠ¥ì„±)
        creation_keywords = ["write", "code", "create", "generate", "function", "script", "class", "impl", "ì‘ì„±", "ë§Œë“¤", "êµ¬í˜„", "ì§œì¤˜"]
        is_creation = any(k in user_lower for k in creation_keywords)
        
        if not is_creation:
            # TOOL í‚¤ì›Œë“œ ë§¤ì¹­
            fast_tools = {
                "get_weather": ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ì˜¨ë„"],
                "search_web": ["ê²€ìƒ‰", "search", "ì •ë³´", "info", "search_web"],
                "search_news": ["ë‰´ìŠ¤", "news", "ìµœì‹ ", "ê¸°ì‚¬", "article", "ì†Œì‹", "ë³´ë„", "ë°œí‘œ", "ê¸°ì‚¬ë“¤", "search_news"],
                "execute_command": ["version", "ë²„ì „", "check", "í™•ì¸", "ì‹¤í–‰", "run", "installed", "ì„¤ì¹˜", "status", "í™˜ê²½"],
                "get_current_time": ["ì‹œê°„", "time", "ëª‡ì‹œ", "date", "ì˜¤ëŠ˜"],
            }
            
            # [Historical Data Fallback]
            # wttr.inì€ ê³¼ê±° ë°ì´í„°ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ê³¼ê±° ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ìœ¼ë¡œ ìœ ë„
            historical_keywords = ["yesterday", "last week", "history", "past", "ì–´ì œ", "ì§€ë‚œ", "ê³¼ê±°", "ì‘ë…„"]
            is_historical = any(k in user_lower for k in historical_keywords)

            for tool_name, keywords in fast_tools.items():
                if any(kw in user_lower for kw in keywords):
                    # ë‚ ì”¨ ì¡°íšŒì¸ë° ê³¼ê±° ë°ì´í„°ë¼ë©´ -> Search Webìœ¼ë¡œ ë³€ê²½
                    if tool_name == "get_weather" and is_historical:
                        return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": "search_web"}

                    # execute_commandì˜ ê²½ìš° ì¶”ê°€ ê²€ì¦
                    if tool_name == "execute_command":
                        # "python version", "check uv" ë“±ì€ í™•ì‹¤í•œ ëª…ë ¹
                        cmd_targets = ["python", "uv", "pip", "node", "npm", "git", "docker", "system", "os"]
                        if any(t in user_lower for t in cmd_targets) or "ls" in user_lower or "dir" in user_lower:
                             # ArgumentëŠ” Orchestrator/Falconì—ê²Œ ìœ„ì„ ("" ì „ë‹¬)
                              return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": tool_name}
                    else:
                        # ArgumentëŠ” Orchestrator/Falconì—ê²Œ ìœ„ì„ ("" ì „ë‹¬)
                        # ì˜ˆ: "ì„œìš¸ ë‚ ì”¨" -> Prompt="" -> Falconì´ "Seoul" ì¶”ì¶œ
                        return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": tool_name}

        # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        if hasattr(self.model, "reset"):
            self.model.reset()
        
        # ChatML í¬ë§· ìˆ˜ë™ êµ¬ì„± (Official Template: <|startoftext|><|im_start|>system...)
        prefix = "<|startoftext|>"
        prompt = f"""{prefix}<|im_start|>system
{ROUTER_SYSTEM_PROMPT}<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""
        
        output = self.model(
            prompt,
            max_tokens=256,
            stop=["<|im_end|>"],
            temperature=self.params["temperature"], # Use dynamic params
            top_p=self.params["top_p"],
            top_k=self.params["top_k"],
            repeat_penalty=self.params["repeat_penalty"],
            echo=False
        )
        
        content = output["choices"][0]["text"].strip()
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            import json
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            start = content.find("{")
            end = content.rfind("}") + 1
            if start >= 0 and end > start:
                result = json.loads(content[start:end])
                return result
        except (json.JSONDecodeError, ValueError):
            pass
        
        # [Fast Path] DIRECT í‚¤ì›Œë“œ ì²´í¬ (ê°•ë ¥ ì¶”ì²œ)
        direct_keywords = ["ìš”ì•½", "ì •ë¦¬", "ì„¤ëª…", "summarize", "explain", "translate", "ë²ˆì—­", "ì•ˆë…•", "hello", "hi", "ë°˜ê°€ì›Œ"]
        if any(kw in user_lower for kw in direct_keywords) and not is_creation:
             return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}

        # REASONER í‚¤ì›Œë“œ (ìˆœìˆ˜ ì½”ë”©ë§Œ)
        keywords_reasoner = ["í•¨ìˆ˜", "ì•Œê³ ë¦¬ì¦˜", "ìˆ˜í•™", "ì¦ëª…", "aime", "fibonacci", "script", "class"]
        
        # 'python'ì´ë‚˜ 'ì½”ë“œ'ê°€ ìˆìœ¼ë©´ REASONER ê°€ëŠ¥ì„± ë†’ìŒ
        if ("python" in user_lower or "ì½”ë“œ" in user_lower or "code" in user_lower) and not any(k in user_lower for k in ["version", "check", "í™•ì¸", "ë²„ì „", "summarize", "ìš”ì•½"]):
             return {"route": "REASONER", "specialist_prompt": user_input, "tool_hint": ""}
             
        if any(kw in user_lower for kw in keywords_reasoner) and not any(kw in user_lower for kw in direct_keywords):
            return {"route": "REASONER", "specialist_prompt": user_input, "tool_hint": ""}
        
        return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
    
    def route_pipeline(self, user_input: str) -> list:
        """
        ë‹¤ì¤‘ ë¼ìš°íŒ… íŒŒì´í”„ë¼ì¸: ë³µí•© ì‘ì—…ì„ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë¶„í•´
        
        ì˜ˆ: "ìµœì‹  AI íŠ¸ë Œë“œ ê²€ìƒ‰í•´ì„œ ìš”ì•½í•´ì¤˜" 
            â†’ [{"route": "TOOL", "tool_hint": "search_web", ...}, 
               {"route": "DIRECT", "task": "ìš”ì•½", ...}]
        
        Returns:
            list of routing decisions (ìˆœì°¨ ì‹¤í–‰)
        """
        import re
        user_lower = user_input.lower()
        
        # ============================================
        # [Step 1] ë³µí•© ì‘ì—… íŒ¨í„´ ê°ì§€
        # ============================================
        
        # íŒ¨í„´: "~í•´ì„œ ~í•´ì¤˜" (ê²€ìƒ‰í•´ì„œ ìš”ì•½í•´ì¤˜, ì°¾ì•„ì„œ ì„¤ëª…í•´ì¤˜)
        # ì£¼ì˜: ë‹¨ìˆœ ìš”ì²­("ì•Œë ¤ì¤˜")ê³¼ ë³µí•© ìš”ì²­("ì•Œë ¤ì£¼ê³  íŒë‹¨í•´ì¤˜")ì„ êµ¬ë¶„í•´ì•¼ í•¨
        compound_patterns = [
            # (TOOL íŠ¸ë¦¬ê±°, í›„ì† DIRECT ì‘ì—…)
            (r'ê²€ìƒ‰.{0,5}(ìš”ì•½|ì •ë¦¬|ì„¤ëª…|ë²ˆì—­)', 'search_web', None),
            (r'ì°¾ì•„.{0,5}(ìš”ì•½|ì •ë¦¬|ì„¤ëª…|ë²ˆì—­)', 'search_web', None),
            # ë‚ ì”¨: "ì•Œë ¤ì£¼ê³  íŒë‹¨í•´" ê°™ì€ ì—°ê²° íŒ¨í„´ë§Œ (ë‹¨ìˆœ "ì•Œë ¤ì¤˜"ëŠ” ì œì™¸)
            (r'ë‚ ì”¨.{0,10}(íŒë‹¨|ì¶”ì²œ|í•„ìš”)', 'get_weather', None),
            (r'ë‚ ì”¨.{0,5}ì•Œë ¤.{0,5}(íŒë‹¨|ì¶”ì²œ|í•„ìš”)', 'get_weather', None),
            (r'ë‰´ìŠ¤.{0,5}(ìš”ì•½|ì •ë¦¬|ë¸Œë¦¬í•‘)', 'search_news', None),
            (r'(ë²„ì „|version).{0,10}(ì„¤ëª…í•´)', 'search_web', None),
            # RAG + ë‚ ì”¨ ë³µí•© íŒ¨í„´: "ë¬¸ì„œ ìš”ì•½í•˜ê³  ë‚ ì”¨ë„ ì•Œë ¤ì¤˜"
            (r'(ìš”ì•½|ì •ë¦¬).{0,15}ë‚ ì”¨.{0,5}(ì•Œë ¤|í™•ì¸)', 'get_weather', 'with_rag'),
            (r'ë‚ ì”¨.{0,5}(ì•Œë ¤|ë„).{0,10}(ìš”ì•½|ì •ë¦¬)', 'get_weather', 'with_rag'),
        ]
        
        # ì˜ì–´ íŒ¨í„´
        compound_patterns_en = [
            (r'search.{0,10}(summarize|explain|translate)', 'search_web', None),
            (r'find.{0,10}(summarize|explain|translate)', 'search_web', None),
            (r'weather.{0,10}(need|should|recommend)', 'get_weather', None),
            (r'news.{0,10}(summarize|brief)', 'search_news', None),
        ]
        
        all_patterns = compound_patterns + compound_patterns_en
        
        for pattern, tool_hint, _ in all_patterns:
            match = re.search(pattern, user_lower)
            if match:
                # í›„ì† ì‘ì—… ì¶”ì¶œ
                follow_up_task = match.group(1) if match.lastindex else "ì²˜ë¦¬"
                
                # íŒŒì´í”„ë¼ì¸ ìƒì„±
                pipeline = [
                    {
                        "route": "TOOL",
                        "specialist_prompt": user_input,
                        "tool_hint": tool_hint,
                        "step": 1,
                        "description": f"{tool_hint} ì‹¤í–‰"
                    },
                    {
                        "route": "DIRECT",
                        "specialist_prompt": "",
                        "tool_hint": "",
                        "step": 2,
                        "description": f"ê²°ê³¼ {follow_up_task}",
                        "context_from_step": 1  # Step 1ì˜ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                    }
                ]
                return pipeline
        
        # ============================================
        # [Step 2] ë³µí•© íŒ¨í„´ ì—†ìœ¼ë©´ ë‹¨ì¼ ë¼ìš°íŒ…
        # ============================================
        single_route = self.route(user_input)
        single_route["step"] = 1
        single_route["description"] = f"{single_route['route']} ë‹¨ì¼ ì‹¤í–‰"
        return [single_route]

    def direct_respond(self, user_input: str, system_prompt: Optional[str] = None) -> str:
        """
        Brainì´ ì§ì ‘ ì‘ë‹µ (ì¼ë°˜ ëŒ€í™”, í•œêµ­ì–´)
        """
        # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” (í•„ìˆ˜: ì´ì „ ìƒíƒœê°€ ë‚¨ìœ¼ë©´ decode ì—ëŸ¬ ë°œìƒ)
        if hasattr(self.model, "reset"):
            self.model.reset()
        
        # ChatML í¬ë§· ìˆ˜ë™ êµ¬ì„±
        # User requested specific default prompt: "You are a helpful assistant trained by Liquid AI."
        sys_content = system_prompt or "You are a helpful assistant trained by Liquid AI. Always respond in Korean unless asked otherwise."
        prefix = "<|startoftext|>"
        prompt = f"""{prefix}<|im_start|>system
{sys_content}<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""
        
        # ì§ì ‘ llm() í˜¸ì¶œ (create_chat_completion ëŒ€ì‹ )
        output = self.model(
            prompt,
            max_tokens=self.n_ctx - 512, # Max context usage
            stop=["<|im_end|>"],
            temperature=self.params["temperature"],
            top_p=self.params["top_p"],
            top_k=self.params["top_k"],
            repeat_penalty=self.params["repeat_penalty"],
            echo=False
        )
        
        return output["choices"][0]["text"].strip()
    
    def integrate_response(self, user_input: str, specialist_output: str) -> str:
        """
        Specialist ì¶œë ¥ì„ ì‚¬ìš©ìì—ê²Œ ë§ê²Œ í†µí•©/í¬ë§·íŒ…
        """
        # Tool outputì´ dict stringì¼ ê²½ìš° ë³´ê¸° ì¢‹ê²Œ ë³€í™˜ ì‹œë„
        formatted_output = specialist_output
        try:
            # [Parsing Strategy]
            # input_data might be a single JSON string OR a multi-task Cowork format:
            # "[TASK: ...]\nDATA: {'...'} \n\n [TASK: ...]"
            
            import re
            
            sections = []
            # Check for Cowork format
            if "[TASK:" in specialist_output and "DATA:" in specialist_output:
                # Split by [TASK: ...] blocks
                raw_sections = re.split(r"\[TASK:.*?\]", specialist_output)
                for raw in raw_sections:
                    if "DATA:" in raw:
                        # Extract JSON part after "DATA:"
                        data_str = raw.split("DATA:", 1)[1].strip()
                        try:
                            data = eval(data_str)
                            sections.append(data)
                        except:
                            # If not a valid python dict/json, treat as plain text
                            # (e.g. Brain summary output)
                            if data_str:
                                sections.append({"type": "text", "content": data_str})
            else:
                # Try parsing as single JSON
                try:
                    data = eval(specialist_output) if "{" in specialist_output else {}
                    if isinstance(data, dict):
                         sections.append(data)
                except:
                    # Treat entire output as text if not JSON
                    sections.append({"type": "text", "content": specialist_output})

            # [Deterministic Formatting]
            final_formatted_blocks = []
            for data in sections:
                if not isinstance(data, dict): continue
                
                # Check for plain text wrapper
                if data.get("type") == "text" and "content" in data:
                    final_formatted_blocks.append(data["content"])
                    continue
                
                # Unwrap 'result' if present (Cowork Tool Result wrapper)
                # {'success': True, 'tool': 'search_news', 'result': {'results': [...]}}
                inner = data.get("result", data) 
                if not isinstance(inner, dict): inner = data # Fallback

                # 1. Search/News Results
                # Check both 'results' (direct) and 'inner["results"]'
                target_data = inner if "results" in inner else data
                
                if "results" in target_data and isinstance(target_data["results"], list):
                    block_lines = []
                    # Add query as header if available
                    q = target_data.get("query", "")
                    if q: block_lines.append(f"results for '{q}':")
                    
                    for item in target_data["results"]:
                        if isinstance(item, dict):
                            title = item.get("title", "No Title")
                            url = item.get("url", item.get("link", ""))
                            snippet = item.get("snippet", item.get("description", ""))
                            # Clean snippet
                            snippet = snippet.replace("\n", " ")[:200]
                            # Format: * Title
                            #           Summary...
                            #           Link: [Click to Read](URL)
                            # Using Markdown link syntax prevents long URL text from wrapping and breaking in TUI.
                            # Rich will render this as a clickable alias.
                            block_lines.append(f"* {title}\n  {snippet}\n  ğŸ”— [Click to Read]({url})")
                    if block_lines:
                        final_formatted_blocks.append("\n".join(block_lines))
                        continue

                # 2. Weather Results
                # {'location': 'Seoul', 'temperature': ...}
                target_data = inner if "temperature" in inner else data
                if "temperature" in target_data and "condition" in target_data:
                    location = target_data.get("location", "City")
                    temp = target_data.get("temperature", "")
                    cond = target_data.get("condition", "")
                    final_formatted_blocks.append(f"* {location} Weather - {temp} / {cond}")
                    continue
                
                # 3. Fallback (Generic Dict)
                fallback_lines = []
                for k, v in target_data.items():
                    if isinstance(v, (str, int, float, bool)):
                        fallback_lines.append(f"- {k}: {v}")
                if fallback_lines:
                    final_formatted_blocks.append("\n".join(fallback_lines))

            if final_formatted_blocks:
                # If we achieved deterministic formatting, return it!
                # This bypasses the Hallucinating Brain.
                return "\n\n".join(final_formatted_blocks)

            # If formatting failed (empty), fallback to original string behavior (Legacy)
            # but usually sections would handle it.
            if not final_formatted_blocks and sections:
                 # Should not happen if sections populated, but just in case
                 formatted_output = str(sections)
        except Exception:
            pass # Continue to LLM if no deterministic output (unlikely for Search/Weather)

        # [English-First Strategy]
        # Generate in English first for speed and quality, then translate later.
        
        system_prompt = f"""You are a formatter. 
Your goal is to fill the provided data into the format below.

[STRICT FORMATTING RULES]
1. OUTPUT IN ENGLISH ONLY. Do NOT translate to Korean here.
2. Use the data provided in the 'Data' section.
3. OUTPUT MUST BE A BULLET LIST.
4. NO INTRO, NO OUTRO.
5. NEVER ALTER URLS. COPY THEM EXACTLY AS IS. Do not remove IDs or query parameters.

[TARGET FORMATS]
For WEATHER:
* City Weather - Temp / Condition
(Use data like 'temperature' and 'condition' from input)

For SEARCH/NEWS:
* Title - Summary (Link: URL)
!!! CRITICAL: YOU MUST INCLUDE THE FULL, EXACT URL FOR EVERY SEARCH RESULT !!!
Format: `* [Title] - [Summary] (Link: [URL])`
Example: `* AI News - content... (Link: https://example.com/article/ar-12345)`

[Data]
{formatted_output}

[User Request]
{user_input}

[Your Output]
""" 

        messages = [
            {"role": "system", "content": "You are a helpful assistant. Output only the formatted list."},
            {"role": "user", "content": system_prompt},
        ]
        
        # [Stability Fix] Reset context
        if hasattr(self.model, "reset"):
            self.model.reset()
        
        # [Performance Optimization] Use INSTRUCT params (Fast, No Thinking)
        # We explicitly use LFM_INSTRUCT_PARAMS here regardless of self.use_thinking
        params = LFM_INSTRUCT_PARAMS.copy()
        
        try:
            response = self.model.create_chat_completion(
                messages=messages,
                max_tokens=params.get("max_tokens", 4096), 
                **params,
            )
            
            return self._clean_response(response["choices"][0]["message"]["content"])
        except Exception as e:
            return f"Error integrating response: {e}"
    
    def _clean_response(self, text: str) -> str:
        """
        Thinking ëª¨ë¸ì˜ <think>...</think> íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ì‹¤ì œ ì‘ë‹µë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        íƒœê·¸ê°€ ë‹«íˆì§€ ì•Šì€ ê²½ìš°(í† í° ë¶€ì¡± ë“±)ì—ë„ ìƒê° ë¶€ë¶„ì„ ìµœëŒ€í•œ ì œê±°í•©ë‹ˆë‹¤.
        """
        import re
        
        # 1. <think>... </think> ì™„ë²½í•œ íƒœê·¸ ì œê±°
        cleaned = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
        
        # 2. ë‹«ëŠ” íƒœê·¸ê°€ ì˜ë¦° ê²½ìš° (<think>ë§Œ ìˆê³  </think>ê°€ ì—†ìŒ)
        if "<think>" in cleaned:
            # <think> ì´í›„ì˜ ëª¨ë“  ë‚´ìš©ì„ ìƒê° ê³¼ì •ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±° (ìƒê°ë§Œ í•˜ë‹¤ê°€ ëë‚œ ê²½ìš°, ë‹µë³€ ì—†ìŒ)
            # ë‹µë³€ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°ê°€ ë˜ë¯€ë¡œ, ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
            return "âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ í† í° ë¶€ì¡±ìœ¼ë¡œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. (Thinking process truncated)"
            
        return cleaned

    def decompose_query(self, user_input: str) -> List[str]:
        """
        ì‚¬ìš©ìì˜ ë³µì¡í•œ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê°œì˜ ê°„ë‹¨í•œ Tool ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
        v4: íœ´ë¦¬ìŠ¤í‹± ì „ìš© (LLM ì œê±°) - ì†ë„ ìµœì í™” + ì •í™•ë„ í–¥ìƒ
        """
        import logging
        import re

        # [Step 0] í† í”½ ìë™ ê°ì§€ (ì •ë°€í™” - ìˆœì„œ ì¤‘ìš”!)
        topic = ""
        topic_keywords = {
            "ë‚ ì”¨": ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ì˜¨ë„"],
            "ë‰´ìŠ¤": ["ë‰´ìŠ¤", "news", "ê¸°ì‚¬", "article", "ì†Œì‹"],
            "ì£¼ê°€": ["ì£¼ê°€", "ì£¼ì‹", "stock", "price"],
            "ì‹œê°„": ["ì‹œê°„", "time", "ëª‡ì‹œ"],
            "ê³„ì‚°": ["ë”í•´", "ë¹¼", "ê³±í•´", "ë‚˜ëˆ ", "ê³„ì‚°", "calculate", "+", "-", "*", "/"],
        }
        
        # í† í”½ ê°ì§€ (ê°€ì¥ ë¨¼ì € ë§¤ì¹­ë˜ëŠ” ê²ƒ ì‚¬ìš©)
        for t, keywords in topic_keywords.items():
            if any(k in user_input.lower() for k in keywords):
                topic = t
                break

        # [Step 1] ë¹„êµ/ì°¨ì´ì  íƒœìŠ¤í¬ ê°ì§€
        has_compare = any(k in user_input.lower() for k in ["ë¹„êµ", "compare", "vs", "ì°¨ì´", "difference"])
        
        # ===============================================
        # [v4] íœ´ë¦¬ìŠ¤í‹± ì „ìš© (ì •ë°€ íŒ¨í„´ ë§¤ì¹­)
        # ===============================================
        
        # Step 1: ë‹¤ì–‘í•œ ì—°ê²°ì–´ íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
        # í•œê¸€: ê³¼, ì™€, ë‘, ì´ë‘, í•˜ê³ 
        # ì˜ì–´: and, or, vs, &
        # ê¸°í˜¸: ,
        split_pattern = r"""
            (?<=[ê°€-í£A-Za-z0-9])(?:ê³¼|ì™€|ë‘|ì´ë‘|í•˜ê³ )\s*  |  # í•œê¸€ ì¡°ì‚¬
            \s*,\s*  |                                        # ì½¤ë§ˆ
            \s+(?:ê·¸ë¦¬ê³ |and|or|vs|ë˜ëŠ”|&)\s+                  # ì—°ê²°ì–´
        """
        parts = re.split(split_pattern, user_input, flags=re.VERBOSE)
        
        # Step 2: ê° íŒŒíŠ¸ì—ì„œ í•µì‹¬ ì—”í‹°í‹° ì¶”ì¶œ
        entities = []
        
        # í™•ì¥ëœ ë¶ˆìš©ì–´
        stopwords = {
            # í•œêµ­ì–´ ë™ì‚¬/ì¡°ì‚¬
            "ë‚ ì”¨", "ë‚ ì”¨ë¥¼", "ë‚ ì”¨ì™€", "ë‚ ì”¨ëŠ”", "ë‰´ìŠ¤", "ë‰´ìŠ¤ë¥¼", "ê²€ìƒ‰", "ê²€ìƒ‰í•´ì¤˜",
            "ë¹„êµí•´ë´", "ë¹„êµ", "ì•Œë ¤ì¤˜", "í•´ì¤˜", "ì°¨ì´ì ", "ì°¨ì´", "ë³´ì—¬ì¤˜",
            "ê·¸ë¦¬ê³ ", "ì˜", "ì„", "ë¥¼", "ê°€", "ì´", "ëŠ”", "ì€", "ì—ì„œ", "ìœ¼ë¡œ", "ì—ê²Œ",
            # ì˜ì–´
            "weather", "news", "search", "compare", "difference", "tell", "show", "me", "the",
            "what", "is", "how", "about", "please", "in", "of", "to", "for", "a", "an",
        }
        
        # í† í”½ í‚¤ì›Œë“œë„ ë¶ˆìš©ì–´ì— ì¶”ê°€
        for keywords in topic_keywords.values():
            for kw in keywords:
                stopwords.add(kw.lower())
        
        for part in parts:
            if not part:
                continue
            part = part.strip()
            
            # ê³µë°±ìœ¼ë¡œ ì¶”ê°€ ë¶„ë¦¬
            words = part.split()
            for word in words:
                word_clean = word.strip()
                
                # í•œêµ­ì–´ ì¡°ì‚¬ ì œê±° (ê¸´ ê²ƒë¶€í„°)
                suffixes_ko = ["ì—ì„œ", "ìœ¼ë¡œ", "ì—ê²Œ", "ì˜", "ë¥¼", "ì„", "ì´", "ê°€", "ì€", "ëŠ”"]
                for suffix in suffixes_ko:
                    if word_clean.endswith(suffix) and len(word_clean) > len(suffix) + 1:
                        word_clean = word_clean[:-len(suffix)]
                        break
                
                # ì˜ì–´ ì†Œìœ ê²© ì œê±°
                if word_clean.endswith("'s"):
                    word_clean = word_clean[:-2]
                
                # ë¶ˆìš©ì–´ ë° ê¸¸ì´ ì²´í¬
                if word_clean and word_clean.lower() not in stopwords and len(word_clean) >= 2:
                    # ìˆ«ì ì²˜ë¦¬: ê³„ì‚° í† í”½ì¼ ë•ŒëŠ” ìˆ«ì ìœ ì§€
                    if word_clean.isdigit() and topic != "ê³„ì‚°":
                        continue
                    entities.append(word_clean)
        
        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        entities = list(dict.fromkeys(entities))
        
        # [Step 3] ê²°ê³¼ ìƒì„±
        if len(entities) >= 1:
            # í† í”½ ë¶™ì´ê¸°
            if topic:
                final_queries = [f"{ent} {topic}" for ent in entities]
            else:
                final_queries = entities.copy()
            
            # ë¹„êµ íƒœìŠ¤í¬ ì¶”ê°€
            if has_compare and len(final_queries) >= 2:
                final_queries.append("Compare results")
                logging.info(f"[Brain] Added compare task")
            
            logging.info(f"[Brain] Heuristic v4: {final_queries}")
            return final_queries
        
        # Fallback: ì›ë³¸ ë°˜í™˜
        return [user_input]
        
        # ===============================================
        # [Fallback] LLM ë¶„í•´ (íœ´ë¦¬ìŠ¤í‹± ì‹¤íŒ¨ ì‹œ)
        # ===============================================
        try:
            # LFM2.5 Chat Template + Few-shot
            prompt = f"""<|startoftext|><|im_start|>system
You extract entities (cities, companies, topics) from queries. Return one entity per line. Do NOT include connectors or topic words.
<|im_end|>
<|im_start|>user
ì„œìš¸ê³¼ ë¶€ì‚° ë‚ ì”¨ ë¹„êµí•´ë´<|im_end|>
<|im_start|>assistant
ì„œìš¸
ë¶€ì‚°<|im_end|>
<|im_start|>user
ì‚¼ì„±ê³¼ ì• í”Œ ë‰´ìŠ¤ ë¹„êµí•´ë´<|im_end|>
<|im_start|>assistant
ì‚¼ì„±
ì• í”Œ<|im_end|>
<|im_start|>user
React, Vue, Angular ì°¨ì´ì <|im_end|>
<|im_start|>assistant
React
Vue
Angular<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""
            # ëª¨ë¸ ë¦¬ì…‹
            if hasattr(self.model, "reset"):
                self.model.reset()
            
            output = self.model(
                prompt,
                max_tokens=32,
                stop=["<|im_end|>", "\n\n"],
                temperature=0.1,  # LFM2.5 ê¶Œì¥
                top_k=50,
                top_p=0.1,
                repeat_penalty=1.05,
                echo=False
            )
            content = output["choices"][0]["text"].strip()
            
            # íŒŒì‹±
            llm_entities = []
            for line in content.split('\n'):
                clean = line.strip().lstrip('-*0123456789. ')
                if clean and len(clean) >= 2 and clean.lower() not in stopwords:
                    llm_entities.append(clean)
            
            if llm_entities:
                if topic:
                    final_queries = [f"{ent} {topic}" for ent in llm_entities]
                else:
                    final_queries = llm_entities
                
                if has_compare and len(final_queries) >= 2:
                    final_queries.append("Compare results")
                
                logging.info(f"[Brain] LLM v3 extracted: {final_queries}")
                return final_queries
                
        except Exception as e:
            logging.error(f"[Brain] LLM Decomposition failed: {e}")
        
        return [user_input]


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("=== Brain í…ŒìŠ¤íŠ¸ ===")
    brain = Brain()
    
    # ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸
    test_inputs = [
        "í”¼ë³´ë‚˜ì¹˜ í•¨ìˆ˜ ì‘ì„±í•´ì¤˜",
        "ì•ˆë…•í•˜ì„¸ìš”!",
        "1 + 1 = ?",
        "AIME 2024 ë¬¸ì œë¥¼ í’€ì–´ë´",
    ]
    
    for inp in test_inputs:
        result = brain.route(inp)
        print(f"Input: {inp}")
        print(f"Route: {result}")
        print()
