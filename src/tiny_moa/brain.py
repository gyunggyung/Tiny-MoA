"""
Brain ëª¨ë¸ ë˜í¼ (LiquidAI LFM2.5-1.2B)
=====================================
- ì˜ë„ ë¶„ì„
- ë¼ìš°íŒ… ê²°ì •
- í•œêµ­ì–´ ì§ì ‘ ì²˜ë¦¬
- ê²°ê³¼ í†µí•©
"""

import os
import re
from pathlib import Path
from typing import List, Optional
from llama_cpp import Llama
import sys
import logging

# [Optimization] Silence llama-cpp logs to keep UI clean
os.environ["LLAMA_CPP_LOG_LEVEL"] = "error" 
logging.getLogger("llama_cpp").setLevel(logging.ERROR)

# LFM2.5 ê¶Œì¥ íŒŒë¼ë¯¸í„° (ê³µì‹ ë¬¸ì„œ: docs.liquid.ai/lfm/inference/llama-cpp)
LFM_INSTRUCT_PARAMS = {
    "temperature": 0.7,
    "top_k": 40,
    "top_p": 0.9,
    "repeat_penalty": 1.1,
}

LFM_THINKING_PARAMS = {
    "temperature": 0.05,  # [Critical] Thinking models require very low temp
    "top_k": 50,
    "top_p": 0.9,
    "repeat_penalty": 1.05,
}

# ë¼ìš°í„° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
ROUTER_SYSTEM_PROMPT = """You are a task router. Analyze the user's request and decide how to handle it.

Available specialists:
- REASONER: STRICTLY for pure coding tasks (writing Python functions/classes) and complex algorithmic/math problems only. Do NOT use for "summarizing", "explaining", "reading files", "checking versions", or "general info".
- TOOL: For requests requiring external information (weather, news, definitions), system status, verify commands, or real-time data.
- DIRECT: For general conversation, summaries, explanations, greetings, translations, and internal knowledge.

Respond with a JSON object:
{"route": "REASONER" or "TOOL" or "DIRECT", "specialist_prompt": "optimized search keywords for TOOL. For 'execute_command', provide the EXACT shell command. Do NOT provide natural language descriptions.", "tool_hint": "tool name if TOOL route"}

Examples:
- "í”¼ë³´ë‚˜ì¹˜ í•¨ìˆ˜ ì‘ì„±í•´ì¤˜" â†’ {"route": "REASONER", "specialist_prompt": "Write a Python function for Fibonacci sequence", "tool_hint": ""}
- "ì´ ë¬¸ì„œ ìš”ì•½í•´ì¤˜" â†’ {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
- "ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ?" â†’ {"route": "TOOL", "specialist_prompt": "Seoul", "tool_hint": "get_weather"}
- "uvê°€ ë­ì•¼?" â†’ {"route": "TOOL", "specialist_prompt": "what is uv python tool", "tool_hint": "search_web"}
"""


class Brain:
    """LFM2.5-1.2B ê¸°ë°˜ Brain ëª¨ë¸"""
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        n_ctx: int = 4096,
        n_threads: Optional[int] = None,
        use_thinking: bool = False,  # PoCì—ì„œ ì‹¤í—˜ í›„ ê²°ì •
    ):
        """
        Args:
            model_path: GGUF ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            n_ctx: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads: CPU ìŠ¤ë ˆë“œ ìˆ˜. Noneì´ë©´ ìë™ ê°ì§€
            use_thinking: Thinking ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ (ì‹¤í—˜ ì¤‘)
        """
        self.use_thinking = use_thinking
        self.params = LFM_THINKING_PARAMS if use_thinking else LFM_INSTRUCT_PARAMS
        
        # ëª¨ë¸ ê²½ë¡œ ê²°ì •
        if model_path is None:
            # 1. ë¡œì»¬ models/ í´ë” í™•ì¸
            base_dir = Path(__file__).parent.parent.parent / "models" / "brain"
            gguf_files = list(base_dir.glob("*.gguf")) if base_dir.exists() else []
            
            if gguf_files:
                model_path = str(gguf_files[0])
            else:
                # 2. HuggingFace ìºì‹œì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ/ì°¾ê¸°
                try:
                    from huggingface_hub import hf_hub_download
                    model_name = "LFM2.5-1.2B-Thinking-Q4_K_M.gguf" if use_thinking else "LFM2.5-1.2B-Instruct-Q4_K_M.gguf"
                    repo_id = "LiquidAI/LFM2.5-1.2B-Thinking-GGUF" if use_thinking else "LiquidAI/LFM2.5-1.2B-Instruct-GGUF"
                    model_path = hf_hub_download(repo_id=repo_id, filename=model_name)
                except Exception as e:
                    raise FileNotFoundError(
                        f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:\n"
                        f"huggingface-cli download LiquidAI/LFM2.5-1.2B-Instruct-GGUF LFM2.5-1.2B-Instruct-Q4_K_M.gguf\n"
                        f"Error: {e}"
                    )
        
        # logger.info(f"[Brain] Loading model from: {model_path}") # Removed print to clean UI
        
        # ìŠ¤ë ˆë“œ ìˆ˜ ê²°ì • (CPU ì½”ì–´ì˜ ì ˆë°˜ ê¶Œì¥)
        if n_threads is None:
            n_threads = max(1, os.cpu_count() // 2)
        
        self.model = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_threads=n_threads,
            verbose=False,
        )
        self.n_ctx = n_ctx
        
        # logger.info(f"[Brain] Loaded! (threads={n_threads}, ctx={n_ctx})") # Removed print to clean UI
    
    def get_prompt_prefix(self) -> str:
        """Returns the prompt prefix (e.g. <|startoftext|>)"""
        return "<|startoftext|>"
    
    def route(self, user_input: str) -> dict:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ë¼ìš°íŒ… ê²°ì •
        
        Returns:
            {"route": "REASONER" | "DIRECT", "specialist_prompt": str}
        """
        user_lower = user_input.lower()
        
        # [Fast Path] í‚¤ì›Œë“œ ê¸°ë°˜ ì¦‰ì‹œ ë¼ìš°íŒ… (LLM í˜¸ì¶œ ì „)
        # ëª…ë°±í•œ ë„êµ¬ ìš”ì²­("ë‚ ì”¨", "ë²„ì „ í™•ì¸")ì€ LLMì„ ê±°ì¹˜ì§€ ì•Šê³  ë°”ë¡œ ì²˜ë¦¬í•˜ì—¬ ì†ë„/ì •í™•ë„ í–¥ìƒ
        
        # 1. ì½”ë”©/ì°½ì‘ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ Fast Path ê±´ë„ˆëœ€ (REASONER ê°€ëŠ¥ì„±)
        creation_keywords = ["write", "code", "create", "generate", "function", "script", "class", "impl", "ì‘ì„±", "ë§Œë“¤", "êµ¬í˜„", "ì§œì¤˜"]
        is_creation = any(k in user_lower for k in creation_keywords)
        
        if not is_creation:
            # TOOL í‚¤ì›Œë“œ ë§¤ì¹­
            fast_tools = {
                "get_weather": ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ì˜¨ë„"],
                "search_web": ["ê²€ìƒ‰", "search", "ì •ë³´", "info", "search_web"],
                "search_news": ["ë‰´ìŠ¤", "news", "ìµœì‹ ", "ê¸°ì‚¬", "article", "ì†Œì‹", "ë³´ë„", "ë°œí‘œ", "ê¸°ì‚¬ë“¤", "search_news"],
                "execute_command": ["version", "ë²„ì „", "check", "í™•ì¸", "ì‹¤í–‰", "run", "installed", "ì„¤ì¹˜", "status", "í™˜ê²½"],
                "get_current_time": ["ì‹œê°„", "time", "ëª‡ì‹œ", "date", "ì˜¤ëŠ˜"],
            }
            
            # [Historical Data Fallback]
            # wttr.inì€ ê³¼ê±° ë°ì´í„°ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ê³¼ê±° ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ìœ¼ë¡œ ìœ ë„
            historical_keywords = ["yesterday", "last week", "history", "past", "ì–´ì œ", "ì§€ë‚œ", "ê³¼ê±°", "ì‘ë…„"]
            is_historical = any(k in user_lower for k in historical_keywords)

            for tool_name, keywords in fast_tools.items():
                if any(kw in user_lower for kw in keywords):
                    # ë‚ ì”¨ ì¡°íšŒì¸ë° ê³¼ê±° ë°ì´í„°ë¼ë©´ -> Search Webìœ¼ë¡œ ë³€ê²½
                    if tool_name == "get_weather" and is_historical:
                        return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": "search_web"}

                    # execute_commandì˜ ê²½ìš° ì¶”ê°€ ê²€ì¦
                    if tool_name == "execute_command":
                        # "python version", "check uv" ë“±ì€ í™•ì‹¤í•œ ëª…ë ¹
                        cmd_targets = ["python", "uv", "pip", "node", "npm", "git", "docker", "system", "os"]
                        if any(t in user_lower for t in cmd_targets) or "ls" in user_lower or "dir" in user_lower:
                             # ArgumentëŠ” Orchestrator/Falconì—ê²Œ ìœ„ì„ ("" ì „ë‹¬)
                              return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": tool_name}
                    else:
                        # ArgumentëŠ” Orchestrator/Falconì—ê²Œ ìœ„ì„ ("" ì „ë‹¬)
                        # ì˜ˆ: "ì„œìš¸ ë‚ ì”¨" -> Prompt="" -> Falconì´ "Seoul" ì¶”ì¶œ
                        return {"route": "TOOL", "specialist_prompt": user_input, "tool_hint": tool_name}

        # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        if hasattr(self.model, "reset"):
            self.model.reset()
        
        # ChatML í¬ë§· ìˆ˜ë™ êµ¬ì„± (Official Template: <|startoftext|><|im_start|>system...)
        prefix = "<|startoftext|>"
        prompt = f"""{prefix}<|im_start|>system
{ROUTER_SYSTEM_PROMPT}<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""
        
        output = self.model(
            prompt,
            max_tokens=256,
            stop=["<|im_end|>"],
            temperature=self.params["temperature"], # Use dynamic params
            top_p=self.params["top_p"],
            top_k=self.params["top_k"],
            repeat_penalty=self.params["repeat_penalty"],
            echo=False
        )
        
        content = output["choices"][0]["text"].strip()
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            import json
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            start = content.find("{")
            end = content.rfind("}") + 1
            if start >= 0 and end > start:
                result = json.loads(content[start:end])
                return result
        except (json.JSONDecodeError, ValueError):
            pass
        
        # [Fast Path] DIRECT í‚¤ì›Œë“œ ì²´í¬ (ê°•ë ¥ ì¶”ì²œ)
        direct_keywords = ["ìš”ì•½", "ì •ë¦¬", "ì„¤ëª…", "summarize", "explain", "translate", "ë²ˆì—­", "ì•ˆë…•", "hello", "hi", "ë°˜ê°€ì›Œ"]
        if any(kw in user_lower for kw in direct_keywords) and not is_creation:
             return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}

        # REASONER í‚¤ì›Œë“œ (ìˆœìˆ˜ ì½”ë”©ë§Œ)
        keywords_reasoner = ["í•¨ìˆ˜", "ì•Œê³ ë¦¬ì¦˜", "ìˆ˜í•™", "ì¦ëª…", "aime", "fibonacci", "script", "class"]
        
        # 'python'ì´ë‚˜ 'ì½”ë“œ'ê°€ ìˆìœ¼ë©´ REASONER ê°€ëŠ¥ì„± ë†’ìŒ
        if ("python" in user_lower or "ì½”ë“œ" in user_lower or "code" in user_lower) and not any(k in user_lower for k in ["version", "check", "í™•ì¸", "ë²„ì „", "summarize", "ìš”ì•½"]):
             return {"route": "REASONER", "specialist_prompt": user_input, "tool_hint": ""}
             
        if any(kw in user_lower for kw in keywords_reasoner) and not any(kw in user_lower for kw in direct_keywords):
            return {"route": "REASONER", "specialist_prompt": user_input, "tool_hint": ""}
        
        return {"route": "DIRECT", "specialist_prompt": "", "tool_hint": ""}
    
    def direct_respond(self, user_input: str, system_prompt: Optional[str] = None) -> str:
        """
        Brainì´ ì§ì ‘ ì‘ë‹µ (ì¼ë°˜ ëŒ€í™”, í•œêµ­ì–´)
        """
        # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” (í•„ìˆ˜: ì´ì „ ìƒíƒœê°€ ë‚¨ìœ¼ë©´ decode ì—ëŸ¬ ë°œìƒ)
        if hasattr(self.model, "reset"):
            self.model.reset()
        
        # ChatML í¬ë§· ìˆ˜ë™ êµ¬ì„±
        # User requested specific default prompt: "You are a helpful assistant trained by Liquid AI."
        sys_content = system_prompt or "You are a helpful assistant trained by Liquid AI. Always respond in Korean unless asked otherwise."
        prefix = "<|startoftext|>"
        prompt = f"""{prefix}<|im_start|>system
{sys_content}<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""
        
        # ì§ì ‘ llm() í˜¸ì¶œ (create_chat_completion ëŒ€ì‹ )
        output = self.model(
            prompt,
            max_tokens=self.n_ctx - 512, # Max context usage
            stop=["<|im_end|>"],
            temperature=self.params["temperature"],
            top_p=self.params["top_p"],
            top_k=self.params["top_k"],
            repeat_penalty=self.params["repeat_penalty"],
            echo=False
        )
        
        return output["choices"][0]["text"].strip()
    
    def integrate_response(self, user_input: str, specialist_output: str) -> str:
        """
        Specialist ì¶œë ¥ì„ ì‚¬ìš©ìì—ê²Œ ë§ê²Œ í†µí•©/í¬ë§·íŒ…
        """
        # Tool outputì´ dict stringì¼ ê²½ìš° ë³´ê¸° ì¢‹ê²Œ ë³€í™˜ ì‹œë„
        formatted_output = specialist_output
        try:
            # [Parsing Strategy]
            # input_data might be a single JSON string OR a multi-task Cowork format:
            # "[TASK: ...]\nDATA: {'...'} \n\n [TASK: ...]"
            
            import re
            
            sections = []
            # Check for Cowork format
            if "[TASK:" in specialist_output and "DATA:" in specialist_output:
                # Split by [TASK: ...] blocks
                raw_sections = re.split(r"\[TASK:.*?\]", specialist_output)
                for raw in raw_sections:
                    if "DATA:" in raw:
                        # Extract JSON part after "DATA:"
                        json_str = raw.split("DATA:", 1)[1].strip()
                        try:
                            data = eval(json_str)
                            sections.append(data)
                        except:
                            pass
            else:
                # Try parsing as single JSON
                try:
                    data = eval(specialist_output) if "{" in specialist_output else {}
                    if isinstance(data, dict):
                         sections.append(data)
                except:
                    pass

            # [Deterministic Formatting]
            final_formatted_blocks = []
            for data in sections:
                if not isinstance(data, dict): continue
                
                # Unwrap 'result' if present (Cowork Tool Result wrapper)
                # {'success': True, 'tool': 'search_news', 'result': {'results': [...]}}
                inner = data.get("result", data) 
                if not isinstance(inner, dict): inner = data # Fallback

                # 1. Search/News Results
                # Check both 'results' (direct) and 'inner["results"]'
                target_data = inner if "results" in inner else data
                
                if "results" in target_data and isinstance(target_data["results"], list):
                    block_lines = []
                    # Add query as header if available
                    q = target_data.get("query", "")
                    if q: block_lines.append(f"results for '{q}':")
                    
                    for item in target_data["results"]:
                        if isinstance(item, dict):
                            title = item.get("title", "No Title")
                            url = item.get("url", item.get("link", ""))
                            snippet = item.get("snippet", item.get("description", ""))
                            # Clean snippet
                            snippet = snippet.replace("\n", " ")[:200]
                            # Format: * Title
                            #           Summary...
                            #           Link: [Click to Read](URL)
                            # Using Markdown link syntax prevents long URL text from wrapping and breaking in TUI.
                            # Rich will render this as a clickable alias.
                            block_lines.append(f"* {title}\n  {snippet}\n  ğŸ”— [Click to Read]({url})")
                    if block_lines:
                        final_formatted_blocks.append("\n".join(block_lines))
                        continue

                # 2. Weather Results
                # {'location': 'Seoul', 'temperature': ...}
                target_data = inner if "temperature" in inner else data
                if "temperature" in target_data and "condition" in target_data:
                    location = target_data.get("location", "City")
                    temp = target_data.get("temperature", "")
                    cond = target_data.get("condition", "")
                    final_formatted_blocks.append(f"* {location} Weather - {temp} / {cond}")
                    continue
                
                # 3. Fallback (Generic Dict)
                fallback_lines = []
                for k, v in target_data.items():
                    if isinstance(v, (str, int, float, bool)):
                        fallback_lines.append(f"- {k}: {v}")
                if fallback_lines:
                    final_formatted_blocks.append("\n".join(fallback_lines))

            if final_formatted_blocks:
                # If we achieved deterministic formatting, return it!
                # This bypasses the Hallucinating Brain.
                return "\n\n".join(final_formatted_blocks)

            # If formatting failed (empty), fallback to original string behavior (Legacy)
            # but usually sections would handle it.
            if not final_formatted_blocks and sections:
                 # Should not happen if sections populated, but just in case
                 formatted_output = str(sections)
        except Exception:
            pass # Continue to LLM if no deterministic output (unlikely for Search/Weather)

        # [English-First Strategy]
        # Generate in English first for speed and quality, then translate later.
        
        system_prompt = f"""You are a formatter. 
Your goal is to fill the provided data into the format below.

[STRICT FORMATTING RULES]
1. OUTPUT IN ENGLISH ONLY. Do NOT translate to Korean here.
2. Use the data provided in the 'Data' section.
3. OUTPUT MUST BE A BULLET LIST.
4. NO INTRO, NO OUTRO.
5. NEVER ALTER URLS. COPY THEM EXACTLY AS IS. Do not remove IDs or query parameters.

[TARGET FORMATS]
For WEATHER:
* City Weather - Temp / Condition
(Use data like 'temperature' and 'condition' from input)

For SEARCH/NEWS:
* Title - Summary (Link: URL)
!!! CRITICAL: YOU MUST INCLUDE THE FULL, EXACT URL FOR EVERY SEARCH RESULT !!!
Format: `* [Title] - [Summary] (Link: [URL])`
Example: `* AI News - content... (Link: https://example.com/article/ar-12345)`

[Data]
{formatted_output}

[User Request]
{user_input}

[Your Output]
""" 

        messages = [
            {"role": "system", "content": "You are a helpful assistant. Output only the formatted list."},
            {"role": "user", "content": system_prompt},
        ]
        
        # [Stability Fix] Reset context
        if hasattr(self.model, "reset"):
            self.model.reset()
        
        # [Performance Optimization] Use INSTRUCT params (Fast, No Thinking)
        # We explicitly use LFM_INSTRUCT_PARAMS here regardless of self.use_thinking
        params = LFM_INSTRUCT_PARAMS.copy()
        
        try:
            response = self.model.create_chat_completion(
                messages=messages,
                max_tokens=params.get("max_tokens", 4096), 
                **params,
            )
            
            return self._clean_response(response["choices"][0]["message"]["content"])
        except Exception as e:
            return f"Error integrating response: {e}"
    
    def _clean_response(self, text: str) -> str:
        """
        Thinking ëª¨ë¸ì˜ <think>...</think> íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ì‹¤ì œ ì‘ë‹µë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        íƒœê·¸ê°€ ë‹«íˆì§€ ì•Šì€ ê²½ìš°(í† í° ë¶€ì¡± ë“±)ì—ë„ ìƒê° ë¶€ë¶„ì„ ìµœëŒ€í•œ ì œê±°í•©ë‹ˆë‹¤.
        """
        import re
        
        # 1. <think>... </think> ì™„ë²½í•œ íƒœê·¸ ì œê±°
        cleaned = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
        
        # 2. ë‹«ëŠ” íƒœê·¸ê°€ ì˜ë¦° ê²½ìš° (<think>ë§Œ ìˆê³  </think>ê°€ ì—†ìŒ)
        if "<think>" in cleaned:
            # <think> ì´í›„ì˜ ëª¨ë“  ë‚´ìš©ì„ ìƒê° ê³¼ì •ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±° (ìƒê°ë§Œ í•˜ë‹¤ê°€ ëë‚œ ê²½ìš°, ë‹µë³€ ì—†ìŒ)
            # ë‹µë³€ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°ê°€ ë˜ë¯€ë¡œ, ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
            return "âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ í† í° ë¶€ì¡±ìœ¼ë¡œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. (Thinking process truncated)"
            
        return cleaned

    def decompose_query(self, user_input: str) -> List[str]:
        """
        ì‚¬ìš©ìì˜ ë³µì¡í•œ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê°œì˜ ê°„ë‹¨í•œ Tool ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
        LLMì„ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì‹¤íŒ¨ ì‹œ ì •ê·œì‹/íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ fallbackí•©ë‹ˆë‹¤.
        """
        import logging
        from typing import List, Optional
        import re # Ensure re is imported if not already

        try:
            # LLM Prompt for Decomposition
            # [Critical Fix] Prevent splitting simple tasks into steps. 
            # We want Tool PARALLELIZATION, not Step-by-Step planning.
            prompt = f"""<|startoftext|>
Task: detailed analysis of whether to split the query.
Query: "{user_input}"

Rules:
1. ONLY split if the user asks for TWO DIFFERENT things (e.g. "Seoul AND Tokyo").
2. Do NOT split a single request into "Check" and "Provide". That is redundant.
3. If it's a single location/topic, return the original query as the ONLY line.

Examples:
"Seoul weather" ->
Seoul weather

"Seoul and Tokyo weather" ->
Seoul weather
Tokyo weather

"Check weather in Seoul" ->
Check weather in Seoul

"Seoul weather?" ->
Seoul weather

User: "{user_input}"
Result:
"""
            # Timeout/Crash ë°©ì§€ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° íŠœë‹
            output = self.model(
                prompt,
                max_tokens=128, 
                stop=["<|im_end|>", "\n\n", "User:", "Task:", "Result:"], 
                temperature=0.0, 
                echo=False
            )
            content = output["choices"][0]["text"].strip()
            
            # Robust Parsing
            lines = []
            for line in content.split('\n'):
                # ìˆ«ì, ë¶ˆë ›, í•˜ì´í”ˆ ë“± ì œê±°
                clean_line = re.sub(r"^[\d\-\*\.]+\s*", "", line.strip()).strip()
                if len(clean_line) > 1: 
                     lines.append(clean_line)
            
            # [Aggressive Filter] If decomposition results in more lines, check if they are just synonyms
            if len(lines) > 1:
                # If original query was short (< 5 words), decomposition is risky unless it has "and/,"
                if len(user_input.split()) < 5 and not any(k in user_input for k in ["and", ",", "ì™€", "ê³¼", "í•˜ê³ ", "vs"]):
                     logging.warning(f"[Brain] Decomposition rejected (Short query, no explicit separator): {lines}")
                     return [user_input]
                     
                logging.info(f"[Brain] LLM Decomposition Success: {lines}")
                return lines
            else:
                 # [Improvement] If LLM failed to split (returned 1 line), 
                 # BUT we detected complex keywords/separators, fall through to Regex/Heuristic below.
                 # Do NOT return [user_input] immediately.
                 logging.info("[Brain] LLM returned 1 line, trying fallback heuristic...")
                 pass 

                
        except Exception as e:
            logging.error(f"[Brain] LLM Decomposition failed: {e}")
            pass
            
        # [Fallback] íœ´ë¦¬ìŠ¤í‹±/Regex ë¶„í•´ (LLM ì‹¤íŒ¨ ì‹œ)
        # "ì„œìš¸ê³¼ ë„ì¿„" -> ["ì„œìš¸", "ë„ì¿„"] -> ["ì„œìš¸ ë‚ ì”¨", "ë„ì¿„ ë‚ ì”¨"] (ë‚ ì”¨ê°€ í¬í•¨ëœ ê²½ìš°)
        import re
        topic = ""
        if any(k in user_input for k in ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ì˜¨ë„"]):
            topic = "ë‚ ì”¨"
        elif any(k in user_input for k in ["ë‰´ìŠ¤", "news", "ê¸°ì‚¬", "article", "ì†Œì‹"]):
            topic = "ë‰´ìŠ¤"
        
        # Regexë¡œ ë¶„ë¦¬ (ì™€/ê³¼/ë‘/ì´ë‘/vs/and/,)
        # [Fix] More inclusive pattern for connectors
        split_pattern = r"(?<=[ê°€-í£])(?:ê³¼|ì™€|ë‘|ì´ë‘)\s+|\s+(?:vs|and|&|or|ë˜ëŠ”|ê·¸ë¦¬ê³ )\s+|\s*,\s*|\s*\?\s*"
        
        parts = re.split(split_pattern, user_input)
        
        # [Fallback Enhancement] "ê´‘ì£¼ ì¶˜ì²œ ë‚ ì”¨" -> ["ê´‘ì£¼", "ì¶˜ì²œ", "ë‚ ì”¨"] -> ["ê´‘ì£¼ ë‚ ì”¨", "ì¶˜ì²œ ë‚ ì”¨"]
        # LLM ì‹¤íŒ¨ ì‹œ, ë‹¨ìˆœ ê³µë°±ìœ¼ë¡œë„ ë¶„ë¦¬ ì‹œë„
        final_parts = []
        for part in parts:
            part = part.strip()
            if not part: continue
            
            # 1. ì´ë¯¸ ì™„ì„±ëœ ë¬¸ì¥ì´ë©´ íŒ¨ìŠ¤
            if len(part.split()) > 3: 
                final_parts.append(part)
                continue
                
            # 2. ê³µë°±ìœ¼ë¡œ ë‚˜ëˆ´ì„ ë•Œ 2ê°œ ì´ìƒì´ê³ , "ë‚ ì”¨" ê°™ì€ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
            sub_parts = part.split()
            if len(sub_parts) >= 2 and any(k in part for k in ["ë‚ ì”¨", "weather", "ê¸°ì˜¨"]):
                # ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ê³µí†µ í‚¤ì›Œë“œì¼ í™•ë¥  ë†’ìŒ (ì˜ˆ: "ì„œìš¸ ëŒ€ì „ ë‚ ì”¨")
                keyword = sub_parts[-1]
                # "ë‚ ì”¨ë¥¼", "ë‚ ì”¨ëŠ”" ë“± ì¡°ì‚¬ê°€ ë¶™ì–´ë„ ì²˜ë¦¬ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •
                if any(x in keyword for x in ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ì˜¨ë„"]):
                    for sub in sub_parts[:-1]:
                        final_parts.append(f"{sub} {keyword}")
                else:
                    # í‚¤ì›Œë“œê°€ ëª…í™•ì§€ ì•Šìœ¼ë©´ ê·¸ëƒ¥ ë‹¤ ë„£ìŒ
                    final_parts.extend(sub_parts)
            else:
                 final_parts.append(part)

        parts = final_parts
        
        # [Filtering] ë¶ˆìš©ì–´ ë° ë¬´ì˜ë¯¸í•œ ì¡°ê° ì œê±°
        filtered_parts = []
        for p in parts:
            p_clean = p.strip()
            # 1. ë„ˆë¬´ ì§§ê±°ë‚˜ íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
            if len(p_clean) < 2: 
                continue
            # 2. "ë‚ ì”¨ë¥¼", "ë¹„êµí•´ì¤˜" ë“± ë¶ˆìš©ì–´ë§Œ ìˆëŠ” ì²­í¬ ì œì™¸
            if p_clean in ["ë‚ ì”¨ë¥¼", "ë‚ ì”¨", "ë¹„êµ", "ë¹„êµí•´ì¤˜", "ì•Œë ¤ì¤˜", "ê·¸ë¦¬ê³ ", "ì†Œê°œí•´ì¤˜", "ê²€ìƒ‰í•´ì¤˜"]:
                continue
            # 3. ì¡°ì‚¬ê°€ ë¶™ì€ ë‹¨ë… í‚¤ì›Œë“œ ì²˜ë¦¬ (ì˜ˆ: "ë‚ ì”¨ëŠ”")
            if p_clean.endswith(("ë‚ ì”¨ë¥¼", "ë‚ ì”¨ëŠ”", "ë‚ ì”¨ê°€")):
                continue
                
            filtered_parts.append(p_clean)
            
        parts = filtered_parts # Update parts with filtered list
        
        if len(parts) > 1:
            # ì •ì œëœ ì¿¼ë¦¬ ìƒì„±
            final_queries = []
            for p in parts:
                # ë¶ˆí•„ìš”í•œ ì„œìˆ ì–´ ì œê±° (ë¹„êµí•´ì¤˜, ì•Œë ¤ì¤˜ ë“±)
                # ì£¼ì˜: "ì–´ë•Œ" ë’¤ì— ì˜¤ëŠ” ë‚´ìš©ì´ ì‚­ì œë˜ë©´ ì•ˆë˜ë¯€ë¡œ .* ì‚¬ìš© ì‹œ ì£¼ì˜.
                # ì´ë¯¸ ë¶„ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ pëŠ” "ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ" í˜•íƒœì¼ ê²ƒì„. ë”°ë¼ì„œ .* ì¨ë„ ë¨.
                clean_p = re.sub(r"(ë¹„êµ|compare|ì•Œë ¤ì¤˜|í•´ì¤˜|ì–´ë•Œ|Check|Verify|with).*", "", p).strip()
                
                if not clean_p: continue
                
                # "ë„ì¿„ ë‚ ì”¨" ì²˜ëŸ¼ ë‚ ì”¨/ë‰´ìŠ¤ê°€ ì´ë¯¸ í¬í•¨ëœ ê²½ìš° ì¤‘ë³µ ë°©ì§€
                if topic and topic not in clean_p and "ë‚ ì”¨" not in clean_p and "ë‰´ìŠ¤" not in clean_p and "ê¸°ì‚¬" not in clean_p:
                     q = f"{clean_p} {topic}".strip()
                else:
                     q = clean_p
                
                if len(q) > 1: # ë„ˆë¬´ ì§§ì€ ì¿¼ë¦¬ ì œì™¸
                     final_queries.append(q)
            
            if len(final_queries) > 1:
                print(f"[Brain] Heuristic Decomposition: {final_queries}")
                return final_queries
            
        return [user_input] # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("=== Brain í…ŒìŠ¤íŠ¸ ===")
    brain = Brain()
    
    # ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸
    test_inputs = [
        "í”¼ë³´ë‚˜ì¹˜ í•¨ìˆ˜ ì‘ì„±í•´ì¤˜",
        "ì•ˆë…•í•˜ì„¸ìš”!",
        "1 + 1 = ?",
        "AIME 2024 ë¬¸ì œë¥¼ í’€ì–´ë´",
    ]
    
    for inp in test_inputs:
        result = brain.route(inp)
        print(f"Input: {inp}")
        print(f"Route: {result}")
        print()
