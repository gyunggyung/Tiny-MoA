"""
Tiny MoA ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
======================
Brainê³¼ Specialistë¥¼ ì¡°ìœ¨í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬
Tool Calling ì§€ì› ì¶”ê°€
"""

import sys
from pathlib import Path
from typing import Optional
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.json import JSON
import re
import threading

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from tiny_moa.brain import Brain
from tiny_moa.reasoner import Reasoner
import logging

# ë²ˆì—­ ëª¨ë“ˆ import
try:
    from translation.pipeline import TranslationPipeline
    from translation.detector import detect_language
    TRANSLATION_AVAILABLE = True
except ImportError:
    TRANSLATION_AVAILABLE = False

console = Console(force_terminal=True, color_system="auto")


class TinyMoA:
    """Tiny MoA (Mixture of Agents) ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(
        self,
        brain_path: Optional[str] = None,
        reasoner_path: Optional[str] = None,
        tool_caller_path: Optional[str] = None,
        n_ctx: int = 4096,
        use_thinking: bool = False,
        show_thinking: bool = False,
        lazy_load: bool = True,
        enable_tools: bool = True,
        enable_translation: bool = True,
    ):
        """
        Args:
            brain_path: Brain ëª¨ë¸ ê²½ë¡œ
            reasoner_path: Reasoner ëª¨ë¸ ê²½ë¡œ
            tool_caller_path: Tool Caller (Falcon-90M) ê²½ë¡œ
            n_ctx: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            use_thinking: LFM Thinking ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ (ì‹¤í—˜ ì¤‘)
            lazy_load: Reasoner/ToolCallerë¥¼ ì²« ì‚¬ìš© ì‹œ ë¡œë“œí• ì§€ ì—¬ë¶€
            enable_tools: Tool Calling ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
        """
        self.brain_path = brain_path
        self.reasoner_path = reasoner_path
        self.tool_caller_path = tool_caller_path
        self.n_ctx = n_ctx
        self.use_thinking = use_thinking
        self.show_thinking = show_thinking
        self.lazy_load = lazy_load
        self.enable_tools = enable_tools
        self.enable_translation = enable_translation and TRANSLATION_AVAILABLE
        
        # ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        self._translation_pipeline = None
        if self.enable_translation:
            try:
                self._translation_pipeline = TranslationPipeline(use_simple_translator=True)
                console.print("[dim]ğŸŒ Translation Pipeline í™œì„±í™”[/dim]")
            except Exception as e:
                console.print(f"[yellow]âš ï¸ ë²ˆì—­ ë¹„í™œì„±í™”: {e}[/yellow]")
                self.enable_translation = False
        
        self._brain: Optional[Brain] = None
        self._reasoner: Optional[Reasoner] = None
        self._tool_caller = None
        self._tool_executor = None
        self.dashboard = None
        self._model_lock = threading.Lock()
        
        console.print("[bold blue]ğŸ¤– Tiny MoA ì´ˆê¸°í™” ì¤‘...[/bold blue]")
        
        # Brainì€ í•­ìƒ ë¡œë“œ (ë¼ìš°í„° ì—­í• )
        self._load_brain()
        
        # Reasoner/ToolCallerëŠ” lazy_load ì„¤ì •ì— ë”°ë¼
        if not lazy_load:
            self._load_reasoner()
            if enable_tools:
                self._load_tool_caller()
        
        console.print("[bold green]âœ… Tiny MoA ì¤€ë¹„ ì™„ë£Œ![/bold green]")
    
    def _load_brain(self):
        """Brain ëª¨ë¸ ë¡œë“œ"""
        if self._brain is None:
            console.print("[dim]Loading Brain (LFM2.5-1.2B)...[/dim]")
            self._brain = Brain(
                model_path=self.brain_path,
                n_ctx=self.n_ctx,
                use_thinking=self.use_thinking,
            )
    
    def _load_reasoner(self):
        """Reasoner ëª¨ë¸ ë¡œë“œ (Lazy)"""
        if self._reasoner is None:
            console.print("[dim]Loading Reasoner (Falcon-R-0.6B)...[/dim]")
            self._reasoner = Reasoner(
                model_path=self.reasoner_path,
                n_ctx=self.n_ctx,
            )
    
    def _load_tool_caller(self):
        """Tool Caller ë¡œë“œ (Lazy)"""
        if self._tool_caller is None and self.enable_tools:
            try:
                from tools.caller import ToolCaller
                from tools.executor import ToolExecutor
                
                console.print("[dim]Loading Tool Caller (Falcon-90M)...[/dim]")
                self._tool_caller = ToolCaller(
                    falcon_path=self.tool_caller_path,
                    brain_model=self._brain,  # Brainìœ¼ë¡œ JSON ë³´ì •
                )
                self._tool_executor = ToolExecutor()
                console.print("[dim]âœ… Tool Caller ì¤€ë¹„ ì™„ë£Œ[/dim]")
            except ImportError as e:
                console.print(f"[yellow]âš ï¸ Tool Calling ë¹„í™œì„±í™”: {e}[/yellow]")
                self.enable_tools = False
    
    @property
    def brain(self) -> Brain:
        if self._brain is None:
            self._load_brain()
        return self._brain
    
    @property
    def reasoner(self) -> Reasoner:
        if self._reasoner is None:
            self._load_reasoner()
        return self._reasoner
    
    @property
    def tool_caller(self):
        if self._tool_caller is None:
            self._load_tool_caller()
        return self._tool_caller
    
    @property
    def tool_executor(self):
        if self._tool_executor is None:
            self._load_tool_caller()
        return self._tool_executor
    
    def _handle_tool_call(self, user_input: str, tool_hint: str = "", arg_hint: str = "", verbose: bool = True, return_raw: bool = False) -> str:
        """
        Tool í˜¸ì¶œ ì²˜ë¦¬
        
        Args:
            return_raw: Trueì¼ ê²½ìš° Brain í†µí•© ì—†ì´ raw result(dict or string) ë°˜í™˜
        """
        if not self.enable_tools or self.tool_executor is None:
            return self.brain.direct_respond(
                user_input,
                system_prompt="The user is asking about real-time information but tools are not available. Apologize and explain."
            )
        
        tool_call = {}
        
        # 1. Brainì´ ì œê³µí•œ ìµœì í™” ì¸ì ì‚¬ìš© (ìš°ì„ ìˆœìœ„ 1)
        if arg_hint and tool_hint:
            if verbose:
                console.print(f"[dim]ğŸ§  Brain ìµœì í™” ì¸ì ì‚¬ìš©: {tool_hint}({arg_hint})[/dim]")
            
            arguments = {}
            if tool_hint in ["search_web", "search_news", "search_wikipedia"]:
                arguments = {"query": arg_hint}
            elif tool_hint == "execute_command":
                # ë°©ì–´ ë¡œì§: ëª…ë ¹ì–´ê°€ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³´ì´ë©´ ë¬´ì‹œí•˜ê³  í‚¤ì›Œë“œ í´ë°± ì‚¬ìš©
                # LFM 1.2Bê°€ ê°€ë” "Check if..." ê°™ì€ ì§€ì‹œë¬¸ì„ ìƒì„±í•¨
                is_valid_cmd = True
                bad_starters = ["Check", "Verify", "Confirm", "Please", "Ensure", "See", "Test", "Determine"]
                
                # 1. ìì—°ì–´ ì‹œì‘ íŒ¨í„´ ì²´í¬
                if any(arg_hint.strip().startswith(s) for s in bad_starters) and len(arg_hint.split()) > 2:
                    is_valid_cmd = False
                
                # 2. í•œê¸€ í¬í•¨ ì—¬ë¶€ ì²´í¬ (ëª…ë ¹ì–´ì— í•œê¸€ì´ ìˆìœ¼ë©´ ìì—°ì–´ ì„¤ëª…ì¼ í™•ë¥  ë†’ìŒ)
                if re.search(r'[ê°€-í£]', arg_hint):
                    is_valid_cmd = False
                
                if is_valid_cmd:
                    arguments = {"command": arg_hint}
                else:
                    if verbose:
                         console.print(f"[yellow]âš ï¸ Brain ìƒì„± ëª…ë ¹ì–´('{arg_hint}')ê°€ ìì—°ì–´ ì„¤ëª…ìœ¼ë¡œ ê°ì§€ë˜ì–´ ë¬´ì‹œí•©ë‹ˆë‹¤. í‚¤ì›Œë“œ ì¶”ë¡ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.[/yellow]")
                    # argumentsë¥¼ ë¹„ì›Œë‘ë©´ ì•„ë˜ìª½ tool_call ìƒì„± ì¡°ê±´(if arguments:)ì„ ë§Œì¡±í•˜ì§€ ëª»í•´
                    # ìì—°ìŠ¤ëŸ½ê²Œ 2. Falcon/í‚¤ì›Œë“œ í´ë°± ë¡œì§ìœ¼ë¡œ ë„˜ì–´ê°
                    arguments = {}
            elif tool_hint == "get_weather":
                arguments = {"location": arg_hint}
            elif tool_hint == "get_current_time":
                arguments = {"timezone": arg_hint}
            elif tool_hint == "calculate":
                arguments = {"expression": arg_hint}
            elif tool_hint == "read_url":
                arguments = {"url": arg_hint}
            
            if arguments:
                tool_call = {"name": tool_hint, "arguments": arguments}
        
        # 2. Tool Callì´ ì•„ì§ ì—†ìœ¼ë©´ Falcon/í‚¤ì›Œë“œ ì‚¬ìš©
        if not tool_call:
            if self.tool_caller and self.tool_caller._falcon:
                # Falcon-90M ì‚¬ìš©
                if verbose:
                    console.print("[dim]ğŸ”§ Tool Caller (Falcon-90M) í˜¸ì¶œ ì¤‘...[/dim]")
                with self._model_lock:
                    tool_call = self.tool_caller.generate_tool_call(user_input)
            else:
                # í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± (ëª¨ë¸ ì—†ì´)
                if verbose:
                    console.print("[dim]ğŸ”§ í‚¤ì›Œë“œ ê¸°ë°˜ Tool ì¶”ë¡  ì¤‘...[/dim]")
                tool_call = self._infer_tool_from_keywords(user_input, tool_hint)
        
        # [Critical Fix] Validate arguments against tool definition to prevent "unexpected keyword" errors
        if tool_call and "name" in tool_call and "arguments" in tool_call:
            t_name = tool_call["name"]
            t_args = tool_call["arguments"]
            
            # search_web does not accept 'location', only 'query'
            if t_name in ["search_web", "search_news"] and "location" in t_args and "query" not in t_args:
                if verbose: console.print(f"[yellow]âš ï¸ Fixing invalid argument for {t_name}: location -> query[/yellow]")
                t_args["query"] = t_args.pop("location")
            
            # get_weather does not accept 'query', only 'location'
            if t_name == "get_weather" and "query" in t_args and "location" not in t_args:
                if verbose: console.print(f"[yellow]âš ï¸ Fixing invalid argument for {t_name}: query -> location[/yellow]")
                t_args["location"] = t_args.pop("query")

        if "error" in tool_call:
            if verbose:
                console.print(f"[yellow]âš ï¸ Tool íŒŒì‹± ì‹¤íŒ¨: {tool_call['error']}[/yellow]")
            return self.brain.direct_respond(user_input)
        
        # 2. Tool ì‹¤í–‰ (Retry Logic)
        tool_name = tool_call.get("name", "")
        arguments = tool_call.get("arguments", {})
        
        if verbose:
            console.print(f"[dim]ğŸ”¨ Tool ì‹¤í–‰: {tool_name}({arguments})[/dim]")
        
        if self.dashboard:
            self.dashboard.add_log(f"API Call: {tool_name}({arguments})", "Tool")
        
        result = self.tool_executor.execute(tool_name, arguments)
        
        if verbose:
            console.print(Panel(
                JSON.from_data(result),
                title=f"[bold cyan]ğŸ”§ {tool_name} { 'ì„±ê³µ' if result.get('success') else 'ì‹¤íŒ¨' }[/bold cyan]",
                border_style="cyan" if result.get("success") else "red",
            ))
        
        # [Semantic Error Detection] Soft Error ê°ì§€
        # íˆ´ì´ ì„±ê³µ(True)í–ˆë‹¤ê³  ë³´ê³ í•´ë„, ë‚´ìš©ì— ì—ëŸ¬ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
        if result.get("success", False):
            raw_result = str(result.get("result", "")).lower()
            error_keywords = ["timeout", "timed out", "rate limit", "api error", "access denied", "404 not found", "500 internal server error", "traceback"]
            
            # ë‹¨, "error"ë¼ëŠ” ë‹¨ì–´ëŠ” ì¼ë°˜ ë¬¸ì¥ì—ë„ ë“¤ì–´ê°ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜ (ì—¬ê¸°ì„œëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ ì œì™¸í•˜ê±°ë‚˜ ë¬¸ë§¥ íŒŒì•… í•„ìš”)
            # í™•ì‹¤í•œ ì‹œìŠ¤í…œ ì—ëŸ¬ í‚¤ì›Œë“œë§Œ ìš°ì„  ì ìš©
            
            for keyword in error_keywords:
                if keyword in raw_result:
                    if verbose:
                        console.print(f"[yellow]âš ï¸ Semantic Error ê°ì§€: '{keyword}' - ì¬ì‹œë„ íŠ¸ë¦¬ê±°[/yellow]")
                    result["success"] = False
                    result["error"] = f"Tool returned success but contained error keyword: {keyword}"
                    break
        
        # 3. Brainìœ¼ë¡œ ê²°ê³¼ í¬ë§·íŒ… or ì¬ì‹œë„
        if result.get("success", False):
            if return_raw:
                 return result # Return full result dict (with tool, arguments, result keys)
            tool_result = result.get("result", {})
            # Brainì˜ integrate_responseë¥¼ ì‚¬ìš©í•˜ì—¬ í™˜ê° ë°©ì§€ ë° í¬ë§·íŒ… ì ìš©
            with self._model_lock:
                return self.brain.integrate_response(user_input, str(tool_result))
        else:
            # Tool ì‹¤íŒ¨ -> ì¬ì‹œë„ (Retry)
            error = result.get("error", "Unknown error")
            
            # ëª¨ë“  Tool ì‹¤íŒ¨ ì‹œ 1íšŒ ì¬ì‹œë„ (Brainì—ê²Œ ìˆ˜ì • ìš”ì²­)
            if "retry" not in arguments: # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if verbose:
                    console.print(f"[bold red]âš ï¸ ì‹¤í–‰ ì‹¤íŒ¨: {error}. Brainì—ê²Œ ìˆ˜ì •ì„ ìš”ì²­í•©ë‹ˆë‹¤...[/bold red]")
                
                # Brainì—ê²Œ ìˆ˜ì •ì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸
                retry_prompt = f"""The tool '{tool_name}' failed with arguments '{arguments}'
Error: "{error}".
The user wants to: "{user_input}".
Please provide CORRECTED arguments for the tool '{tool_name}' to fix this error.
Return ONLY the JSON arguments (e.g. {{"location": "Seoul"}} or {{"command": "python --version"}}). Do NOT explain."""

                with self._model_lock:
                    corrected_args_str = self.brain.direct_respond(
                        retry_prompt, 
                        system_prompt="You are a tool expert. Provide only the corrected JSON arguments."
                    ).strip()
                
                # ë§ˆí¬ë‹¤ìš´/JSON íŒŒì‹± ì‹œë„
                corrected_args_str = corrected_args_str.replace("```json", "").replace("```", "").strip()
                
                try:
                    import json
                    # ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš°(ì˜ˆ: command string) ì²˜ë¦¬
                    if not corrected_args_str.startswith("{"):
                         # execute_commandë¼ë©´ ë¬¸ìì—´ì„ commandë¡œ ê°„ì£¼
                         if tool_name == "execute_command":
                             retry_args = {"command": corrected_args_str}
                         else:
                             # ë‹¤ë¥¸ íˆ´ì€ location ë“± í‚¤ë¥¼ ì•Œê¸° ì–´ë ¤ìš°ë¯€ë¡œ JSON íŒŒì‹± ì¬ì‹œë„í•˜ê±°ë‚˜ í¬ê¸°
                             # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ locationì´ë‚˜ queryë¡œ ê°€ì •í•˜ëŠ” íœ´ë¦¬ìŠ¤í‹± ì¶”ê°€ ê°€ëŠ¥í•˜ë‚˜,
                             # Brainì´ JSONì„ ì£¼ë„ë¡ í”„ë¡¬í”„íŠ¸í–ˆìœ¼ë¯€ë¡œ ì¼ë‹¨ JSON ë¡œë“œ ì‹œë„
                             pass
                    
                    if corrected_args_str.startswith("{"):
                        retry_args = json.loads(corrected_args_str)
                        retry_args["retry"] = True # ì¬ê·€ ë°©ì§€ í”Œë˜ê·¸
                        
                        if verbose:
                            console.print(f"[dim]ğŸ§  Brain ìˆ˜ì • ì œì•ˆ: {retry_args}[/dim]")

                        retry_result = self.tool_executor.execute(tool_name, retry_args)
                        
                        if verbose:
                            console.print(Panel(
                                JSON.from_data(retry_result),
                                title=f"[bold cyan]ğŸ”§ ì¬ì‹œë„ ê²°ê³¼[/bold cyan]",
                                border_style="cyan" if retry_result.get("success") else "red",
                            ))
                            
                        if retry_result.get("success"):
                            # ì„±ê³µ ì‹œ í¬ë§·íŒ… í›„ ë°˜í™˜
                            tool_result = retry_result.get("result", {})
                            if return_raw:
                                 return retry_result
                            # Brainì˜ integrate_responseë¥¼ ì‚¬ìš©í•˜ì—¬ í™˜ê° ë°©ì§€ ë° í¬ë§·íŒ… ì ìš©
                            with self._model_lock:
                                return self.brain.integrate_response(user_input, str(tool_result))
                        else:
                            error = retry_result.get("error", error)
                except Exception as e:
                    if verbose:
                        console.print(f"[dim]âš ï¸ ì¬ì‹œë„ íŒŒì‹± ì‹¤íŒ¨: {e}[/dim]")

            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ëª…ë ¹ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\nì˜¤ë¥˜: {error}"
    
    def _infer_tool_from_keywords(self, user_input: str, tool_hint: str = "") -> dict:
        """í‚¤ì›Œë“œ ê¸°ë°˜ Tool í˜¸ì¶œ ì¶”ë¡  (ëª¨ë¸ ì—†ì´)"""
        user_lower = user_input.lower()
        
        # tool_hintê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if tool_hint == "get_weather":
            # ë„ì‹œëª… ì¶”ì¶œ ì‹œë„
            cities = ["ì„œìš¸", "seoul", "ë„ì¿„", "tokyo", "ë‰´ìš•", "new york", "ëŸ°ë˜", "london", 
                      "ë¶€ì‚°", "busan", "ì¸ì²œ", "ëŒ€êµ¬", "ëŒ€ì „", "ê´‘ì£¼"]
            location = "Seoul"  # ê¸°ë³¸ê°’
            for city in cities:
                if city in user_lower:
                    location = city.title()
                    break
            return {"name": "get_weather", "arguments": {"location": location}}
        
        elif tool_hint == "search_web":
            # ê²€ìƒ‰ì–´ ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            query = user_input
            for prefix in ["ê²€ìƒ‰í•´ì¤˜", "ì°¾ì•„ë´", "ì•Œë ¤ì¤˜", "ë­ì•¼", "search for", "search"]:
                if prefix in user_lower:
                    query = user_input.replace(prefix, "").strip()
                    break
            return {"name": "search_web", "arguments": {"query": query}}
        
        elif tool_hint == "get_current_time":
            # íƒ€ì„ì¡´ ì¶”ì¶œ
            timezone = "Asia/Seoul"  # ê¸°ë³¸ê°’
            if "ë‰´ìš•" in user_lower or "new york" in user_lower:
                timezone = "America/New_York"
            elif "ë„ì¿„" in user_lower or "tokyo" in user_lower:
                timezone = "Asia/Tokyo"
            elif "ëŸ°ë˜" in user_lower or "london" in user_lower:
                timezone = "Europe/London"
            return {"name": "get_current_time", "arguments": {"timezone": timezone}}
        
        elif tool_hint == "calculate":
            # ìˆ˜ì‹ ì¶”ì¶œ
            import re
            match = re.search(r'[\d\s+\-*/().]+', user_input)
            expression = match.group().strip() if match else "0"
            return {"name": "calculate", "arguments": {"expression": expression}}
        
        # tool_hint ì—†ì„ ë•Œ í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± (ì˜ì–´ í‚¤ì›Œë“œ í¬í•¨)
        weather_keywords = ["weather", "ë‚ ì”¨", "ê¸°ì˜¨", "ì˜¨ë„", "temperature"]
        search_keywords = ["search", "find", "ê²€ìƒ‰", "ì°¾ì•„", "ì•Œë ¤ì¤˜"]
        time_keywords = ["time", "ì‹œê°„", "ëª‡ì‹œ", "what time", "current time"]
        
        if any(kw in user_lower for kw in weather_keywords):
            # ë„ì‹œëª… ì¶”ì¶œ
            cities = ["seoul", "ì„œìš¸", "tokyo", "ë„ì¿„", "new york", "ë‰´ìš•", "london", "ëŸ°ë˜",
                      "busan", "ë¶€ì‚°", "incheon", "ì¸ì²œ", "osaka", "ì˜¤ì‚¬ì¹´"]
            location = "Seoul"
            for city in cities:
                if city in user_lower:
                    location = city.title().replace("ì„œìš¸", "Seoul").replace("ë„ì¿„", "Tokyo").replace("ë‰´ìš•", "New York").replace("ëŸ°ë˜", "London").replace("ë¶€ì‚°", "Busan")
                    break
            return {"name": "get_weather", "arguments": {"location": location}}
        
        command_keywords = ["ì‹¤í–‰", "run", "check", "verify", "version", "ë²„ì „", "í™•ì¸", "ls", "dir", "command"]
        if any(kw in user_lower for kw in command_keywords) and ("ì½”ë“œ" not in user_lower):
             # ê°„ë‹¨í•œ ëª…ë ¹ì–´ ì¶”ì¶œ ì‹œë„ (ë§¤ìš° ë‹¨ìˆœí™”ë¨)
            cmd = "ver" # ê¸°ë³¸ê°’
            if "uv" in user_lower:
                 cmd = "uv --version"
            elif "python" in user_lower:
                 cmd = "python --version"
            elif "dir" in user_lower or "ëª©ë¡" in user_lower:
                 cmd = "dir"
            return {"name": "execute_command", "arguments": {"command": cmd}}

        if any(kw in user_lower for kw in search_keywords) or "uv" in user_lower:
            return {"name": "search_web", "arguments": {"query": user_input}}
        
        if any(kw in user_lower for kw in time_keywords):
            return {"name": "get_current_time", "arguments": {"timezone": "Asia/Seoul"}}
        
        return {"error": "Could not infer tool from keywords"}
    
    def _process_rag_attachments(self, user_input: str, verbose: bool = True) -> tuple[str, str]:
        """
        @[filename] íŒ¨í„´ì„ ì°¾ì•„ RAG ì²˜ë¦¬í•˜ê³  ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜
        
        Returns:
            (cleaned_user_input, rag_context)
        """
        rag_context = ""
        rag_files = re.findall(r"@\[(.*?)\]", user_input)
        
        if not rag_files:
            return user_input, ""
            
        if verbose:
            console.print(f"[dim]ğŸ“š RAG íŒŒì¼ ê°ì§€: {rag_files}[/dim]")
        
        # Lazy Loading check
        if not hasattr(self, "_rag_engine") or self._rag_engine is None:
            try:
                from src.rag.engine import RAGEngine
                self._rag_engine = RAGEngine()
            except ImportError as e:
                    console.print(f"[red]âš ï¸ RAG Engine ë¡œë“œ ì‹¤íŒ¨: {e}[/red]")
                    self._rag_engine = None

        if self._rag_engine:
            for file_ref in rag_files:
                # íŒŒì¼ ê²½ë¡œ ë³´ì • (í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
                file_path = file_ref.strip()
                if not Path(file_path).exists():
                        # í˜¹ì‹œ ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹ˆë¼ë©´ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
                        file_path = str(Path(project_root) / file_ref.strip())
                
                if Path(file_path).exists():
                    # 1. Ingest (ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ìŠ¤í‚µë¨ - Engine ë‚´ë¶€ ë¡œì§)
                    if verbose:
                            console.print(f"[dim]ğŸ”„ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: {Path(file_path).name}...[/dim]")
                    status = self._rag_engine.ingest_file(file_path)
                    if verbose:
                            console.print(f"[dim]   Result: {status}[/dim]")
                    
                    # 2. Query (ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš© ê²€ìƒ‰)
                    # ì§ˆë¬¸ì—ì„œ íŒŒì¼ ì°¸ì¡° ì œê±° í›„ ê²€ìƒ‰
                    clean_query = re.sub(r"@\[(.*?)\]", "", user_input).strip()
                    retrieved = self._rag_engine.query(clean_query)
                    
                    if retrieved:
                            rag_context += f"\n\n[Context from {file_ref}]\n{retrieved}\n"
                else:
                    if verbose:
                            console.print(f"[yellow]âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_ref}[/yellow]")
        
        # ì…ë ¥ì—ì„œ íŒŒì¼ ì°¸ì¡° ì œê±°
        clean_input = re.sub(r"@\[(.*?)\]", "", user_input).strip()
        
        if rag_context and verbose:
            console.print(f"[dim]ğŸ“„ RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ë¨ ({len(rag_context)} chars)[/dim]")
            
        return clean_input, rag_context

    def chat(self, user_input: str, rag_context: str = "", verbose: bool = True, return_raw_tool_result: bool = False) -> str:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ì‘ë‹µ (Thinking -> Tool Calling -> RAG -> Brain ìˆœìœ„)
        
        Args:
            return_raw_tool_result: Trueì¼ ê²½ìš° Tool í˜¸ì¶œ ê²°ê³¼ë¥¼ Brain í†µí•© ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        """
        """
        ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
        
        Args:
            user_input: ì‚¬ìš©ì ë©”ì‹œì§€
            verbose: ì²˜ë¦¬ ê³¼ì • ì¶œë ¥ ì—¬ë¶€
            
        Returns:
            ìµœì¢… ì‘ë‹µ
        """
        if verbose:
            console.print(f"\n[bold]ğŸ“ ì…ë ¥:[/bold] {user_input}")

        # 0.1. [RAG] íŒŒì¼ ì°¸ì¡° ê°ì§€ (@[filename])
        user_input, rag_context = self._process_rag_attachments(user_input, verbose=verbose)
        if rag_context:
             user_input += f"\n\n--- Reference Material ---\n{rag_context}\n--------------------------\n(Answer strictly based on the Reference Material above if relevant.)"
        
        # 0.5. [Multi-Step Pipeline] route_pipeline() ì‚¬ìš©í•˜ì—¬ ë³µí•© ì‘ì—… ë¶„í•´
        # ì˜ˆ: "ìµœì‹  AI íŠ¸ë Œë“œ ê²€ìƒ‰í•´ì„œ ìš”ì•½í•´ì¤˜" â†’ [TOOL: search] â†’ [DIRECT: ìš”ì•½]
        pipeline = self.brain.route_pipeline(processed_input if 'processed_input' in dir() else user_input)
        
        if len(pipeline) > 1:
            # ë‹¤ì¤‘ ìŠ¤í… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            if verbose:
                console.print(f"[dim]ğŸ”— íŒŒì´í”„ë¼ì¸ ê°ì§€: {len(pipeline)}ë‹¨ê³„ ì‹¤í–‰[/dim]")
                for step in pipeline:
                    console.print(f"[dim]   Step {step['step']}: {step['route']} - {step.get('description', '')}[/dim]")
            
            step_results = {}  # ê° ìŠ¤í…ì˜ ê²°ê³¼ ì €ì¥
            
            for step in pipeline:
                step_num = step["step"]
                route = step["route"]
                tool_hint = step.get("tool_hint", "")
                
                if verbose:
                    console.print(f"[dim]â–¶ Step {step_num}: {route}[/dim]")
                
                if route == "TOOL":
                    # Tool ì‹¤í–‰
                    result = self._handle_tool_call(
                        user_input, 
                        tool_hint, 
                        step.get("specialist_prompt", ""), 
                        verbose=verbose,
                        return_raw=True  # Raw ê²°ê³¼ í•„ìš”
                    )
                    step_results[step_num] = result
                    
                elif route == "DIRECT":
                    # ì´ì „ ìŠ¤í…ì˜ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                    context_from = step.get("context_from_step", step_num - 1)
                    prev_result = step_results.get(context_from, "")
                    
                    # ê²°ê³¼ í¬ë§·íŒ…
                    if isinstance(prev_result, dict):
                        prev_result = str(prev_result.get("result", prev_result))
                    
                    # Brainì—ê²Œ ìš”ì•½/ì²˜ë¦¬ ìš”ì²­
                    with self._model_lock:
                        final_response = self.brain.integrate_response(user_input, str(prev_result))
                    step_results[step_num] = final_response
                    
                elif route == "REASONER":
                    with self._model_lock:
                        result = self.reasoner.solve(step.get("specialist_prompt", user_input))
                    step_results[step_num] = result
            
            # ë§ˆì§€ë§‰ ìŠ¤í… ê²°ê³¼ ë°˜í™˜
            final_response = step_results.get(len(pipeline), list(step_results.values())[-1])
            
            if verbose:
                console.print(Panel(
                    Markdown(str(final_response)) if isinstance(final_response, str) else JSON.from_data(final_response),
                    title="[bold green]ğŸ”— íŒŒì´í”„ë¼ì¸ ì™„ë£Œ[/bold green]",
                    border_style="green",
                ))
            
            # ë²ˆì—­ ì²˜ë¦¬ (í•„ìš”ì‹œ)
            if self.enable_translation and self._translation_pipeline and isinstance(final_response, str):
                try:
                    target_lang_ctx = self._translation_pipeline.to_english(user_input)
                    if target_lang_ctx.is_translated:
                        final_response = self._translation_pipeline.from_english(final_response, target_lang_ctx)
                except Exception as e:
                    logger.error(f"Pipeline translation failed: {e}")
            
            return final_response
        
        # 0.5.1 [Legacy] ê¸°ì¡´ ë³µí•© ì§ˆë¬¸ ë¶„í•´ (compare/ë¹„êµ ì¼€ì´ìŠ¤)
        # "ë¹„êµ", "compare", "vs" ë“± í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¶„í•´ ì‹œë„
        complex_keywords = ["ë¹„êµ", "compare", "vs", "difference", "ì°¨ì´", "ì–´ë•Œ?", "ê°ê°", "separately", "each"] # 'ì–´ë•Œ?'ëŠ” ì• ë§¤í•˜ì§€ë§Œ ì¼ë‹¨ í…ŒìŠ¤íŠ¸
        is_complex = any(k in user_input for k in ["ë¹„êµ", "compare", "vs", "difference", "ì°¨ì´", "ê°ê°", "separately", "each"])
        
        if is_complex:
            if verbose:
                console.print("[dim]ğŸ§© ë³µí•© ì§ˆë¬¸ ê°ì§€: ë¶„í•´ ì‹œë„ ì¤‘...[/dim]")
            
            sub_queries = self.brain.decompose_query(user_input)
            
            # ë¶„í•´ê°€ ì‹¤ì œë¡œ ì¼ì–´ë‚¬ëŠ”ì§€ í™•ì¸ (1ê°œ ì´ìƒì´ê³ , ì›ë³¸ê³¼ ë‹¤ì„ ë•Œ)
            if len(sub_queries) > 1:
                if verbose:
                    console.print(f"[dim]ğŸ§© ë¶„í•´ ê²°ê³¼: {sub_queries}[/dim]")
                
                context_results = []
                for sub_q in sub_queries:
                    # ê° í•˜ìœ„ ì§ˆë¬¸ ì²˜ë¦¬
                    # ì¬ê·€ í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ë‹¨ìˆœ ì²˜ë¦¬ ë¡œì§ í•„ìš”í•˜ë‚˜, ì—¬ê¸°ì„œëŠ” chat() í˜¸ì¶œí•˜ë˜
                    # ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•´ is_complex ì²´í¬ê°€ ì¤‘ìš”í•¨.
                    # í•˜ì§€ë§Œ sub_qëŠ” ë‹¨ìˆœí•  ê²ƒì´ë¯€ë¡œ ê´œì°®ìŒ.
                    # ë‹¤ë§Œ chat()ì€ ë²ˆì—­/ì¶œë ¥ì„ ë˜ í•˜ë¯€ë¡œ, ë‚´ë¶€ í•¨ìˆ˜ _process_single_turn ê°™ì€ê²Œ í•„ìš”.
                    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ: route -> handle_tool_call ë³µë¶™ ë¡œì§ ì‚¬ìš© (í•¨ìˆ˜ ë¶„ë¦¬ ê¶Œì¥í•˜ì§€ë§Œ ì¼ë‹¨ ì¸ë¼ì¸)
                    
                    # 1. Brainì´ ë¼ìš°íŒ… ê²°ì • (Sub query)
                    # ë²ˆì—­ í•„ìš”ì‹œ ë²ˆì—­
                    sub_processed = sub_q
                    if self.enable_translation and self._translation_pipeline:
                        t_ctx = self._translation_pipeline.to_english(sub_q)
                        if t_ctx.is_translated:
                            sub_processed = t_ctx.english_text

                    route_result = self.brain.route(sub_processed)
                    route = route_result.get("route", "DIRECT")
                    
                    step_result = ""
                    if route == "TOOL":
                         tool_hint = route_result.get("tool_hint", "")
                         arg_hint = route_result.get("specialist_prompt", "")
                         # Tool ì‹¤í–‰ ë° ê²°ê³¼ íšë“ (í¬ë§·íŒ… ì „ì˜ Raw Resultê°€ í•„ìš”í•˜ì§€ë§Œ, _handle_tool_callì€ í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ ë°˜í™˜)
                         # ì—¬ê¸°ì„  _handle_tool_callì˜ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                         step_result = self._handle_tool_call(sub_q, tool_hint, arg_hint, verbose=True)
                    else:
                         step_result = self.brain.direct_respond(sub_processed)
                    
                    
                    context_results.append(f"Query: {sub_q}\nResult: {step_result[:500]}") # ê²°ê³¼ ê¸¸ì´ ì œí•œ (500ì)
                
                # ê²°ê³¼ í†µí•©
                aggregated_context = "\n\n".join(context_results)
                
                # í†µí•© í˜¸ì¶œ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°„ì ‘ì )
                if hasattr(self.brain.model, "reset"):
                    self.brain.model.reset()
                    
                final_response = self.brain.integrate_response(user_input, aggregated_context)
                
                if verbose:
                    console.print(Panel(
                        Markdown(final_response),
                        title="[bold green]ğŸ’¬ í†µí•© ì‘ë‹µ[/bold green]",
                        border_style="green",
                    ))
                
                # ë²ˆì—­: en â†’ original_lang (ìˆë‹¤ë©´)
                # ì£¼ì˜: decomposition ë¡œì§ ì‹œì‘ ì „ì— translation_ctxë¥¼ êµ¬í–ˆì–´ì•¼ í•¨.
                # í•˜ì§€ë§Œ êµ¬ì¡°ìƒ chat í•¨ìˆ˜ì˜ ë©”ì¸ íŒŒì´í”„ë¼ì¸(0ë²ˆ ë‹¨ê³„)ë³´ë‹¤ ë¨¼ì € ì‹¤í–‰ë¨.
                # ë”°ë¼ì„œ ì—¬ê¸°ì„œ ë³„ë„ë¡œ detect/translate í•˜ê±°ë‚˜, 0ë²ˆ ë‹¨ê³„ë¥¼ ìœ„ë¡œ ì˜¬ë ¤ì•¼ í•¨.
                # ë¦¬íŒ©í† ë§ ìµœì†Œí™”ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œ ê°„ë‹¨íˆ ì²˜ë¦¬.
                
                # (ì´ë¯¸ chat í•¨ìˆ˜ ì§„ì… ì‹œì ì—ëŠ” processed_inputì´ ì—†ìœ¼ë¯€ë¡œ, user_inputì„ ì´ìš©)
                # [Critical Fix] If raw output is requested, skip ALL translation logic to avoid dict vs string errors
                if return_raw_tool_result:
                    return final_response

                if self.enable_translation and self._translation_pipeline and isinstance(final_response, str):
                    # Brain now outputs English, so we MUST translate to original language.
                    # Attempt to detect language from the original user_input.
                    try:
                        # Re-detect context if not available (since this is inside decomposition block)
                        target_lang_ctx = self._translation_pipeline.to_english(user_input)
                        
                        if target_lang_ctx.is_translated:
                            # User spoke non-English (e.g. Korean), translate back
                            final_response = self._translation_pipeline.from_english(final_response, target_lang_ctx)
                        else:
                            # User spoke English (or detection failed).
                            # If the system policy enforces Korean, we might consider forcing translation here.
                            # However, the standard logic is to respect the input language or explicit user instruction.
                            # For now, we maintain the "Return in Original Language" policy.
                            pass

                    except Exception as e:
                        logger.error(f"Translation logic failed: {e}")

                return final_response

        # 0. ë²ˆì—­ íŒŒì´í”„ë¼ì¸: ë‹¤êµ­ì–´ â†’ ì˜ì–´
        translation_ctx = None
        processed_input = user_input
        
        if self.enable_translation and self._translation_pipeline:
            with self._model_lock:
                translation_ctx = self._translation_pipeline.to_english(user_input)
            if translation_ctx.is_translated:
                processed_input = translation_ctx.english_text
                if verbose:
                    console.print(f"[dim]ğŸŒ ë²ˆì—­: {translation_ctx.original_lang} â†’ en[/dim]")
                    console.print(f"[dim]   ì˜ì–´: {processed_input[:50]}...[/dim]")
        
        # 1. Brainì´ ë¼ìš°íŒ… ê²°ì • (ì˜ì–´ë¡œ ëœ ì…ë ¥ ì‚¬ìš©)
        # [Fix] RAG ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ Tool Callingì„ ë°©ì§€í•˜ê³  ê°•ì œë¡œ DIRECT ì‘ë‹µ ìœ ë„
        if rag_context:
             if verbose:
                 console.print("[dim]ğŸ“„ RAG ì»¨í…ìŠ¤íŠ¸ ì¡´ì¬: ê°•ì œë¡œ DIRECT ëª¨ë“œ ì „í™˜[/dim]")
             route = "DIRECT"
             specialist_prompt = ""
             tool_hint = ""
        else:
             with self._model_lock:
                 route_result = self.brain.route(processed_input)
             route = route_result.get("route", "DIRECT")
             specialist_prompt = route_result.get("specialist_prompt", "")
             tool_hint = route_result.get("tool_hint", "")
        
        if verbose:
            console.print(f"[dim]ğŸ§  ë¼ìš°íŒ…: {route}[/dim]")
        
        # 2. ë¼ìš°íŒ…ì— ë”°ë¥¸ ì²˜ë¦¬
        if route == "TOOL":
            # Tool Calling
            if verbose:
                console.print(f"[dim]ğŸ”§ Tool í˜¸ì¶œ: {tool_hint}[/dim]")
            # specialist_promptë¥¼ arg_hintë¡œ ì „ë‹¬
            # [Critical Fix] Use processed_input (EN) instead of user_input (KO) for tool calling
            final_response = self._handle_tool_call(processed_input, tool_hint, specialist_prompt, verbose, return_raw=return_raw_tool_result)
            
        elif route == "REASONER" and specialist_prompt:
            # Reasoner í˜¸ì¶œ
            if verbose:
                console.print("[dim]ğŸ¤” Reasoner í˜¸ì¶œ ì¤‘...[/dim]")
            
            with self._model_lock:
                specialist_output = self.reasoner.solve(specialist_prompt)
            
            # PoC: Reasoner ì¶œë ¥ ì§ì ‘ ë°˜í™˜ (í† í° ì ˆì•½)
            final_response = specialist_output
        else:
            # Brainì´ ì§ì ‘ ì‘ë‹µ
            if verbose:
                console.print("[dim]ğŸ§  Brain ì§ì ‘ ì‘ë‹µ...[/dim]")
            with self._model_lock:
                final_response = self.brain.direct_respond(processed_input)
        
        # 3. ë²ˆì—­ íŒŒì´í”„ë¼ì¸: ì˜ì–´ â†’ ì›ë˜ ì–¸ì–´
        # [Fix] Raw ê²°ê³¼(dict)ëŠ” ë²ˆì—­í•˜ì§€ ì•ŠìŒ + íƒ€ì… ì²´í¬ ê°•ì œ
        if not return_raw_tool_result and isinstance(final_response, str) and translation_ctx and translation_ctx.is_translated and self._translation_pipeline:
            if verbose:
                console.print(f"[dim]ğŸŒ ë²ˆì—­: en â†’ {translation_ctx.original_lang}[/dim]")
            with self._model_lock:
                try:
                    final_response = self._translation_pipeline.from_english(final_response, translation_ctx)
                except Exception as e:
                    logger.error(f"Translation failed (main): {e}")
        if verbose:
            console.print(Panel(
                Markdown(str(final_response)) if isinstance(final_response, str) else JSON.from_data(final_response),
                title="[bold green]ğŸ’¬ ì‘ë‹µ[/bold green]",
                border_style="green",
            ))
            
            # [Thinking Model Visualization]
            # ë§Œì•½ Thinking Traceê°€ í¬í•¨ëœ ê²½ìš° (ì˜ˆ: <thinking>...</thinking> ë˜ëŠ” ìœ ì‚¬ íŒ¨í„´)
            # ë³„ë„ë¡œ íŒŒì‹±í•˜ì—¬ ë³´ì—¬ì£¼ëŠ” ë¡œì§ ì¶”ê°€
            if self.use_thinking and self.show_thinking and isinstance(final_response, str):
                 # Thinking Traceê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ìˆìœ¼ë©´ ë³„ë„ íŒ¨ë„ë¡œ ì¶œë ¥
                 # (í˜„ì¬ ëª¨ë¸ì€ ëª…ì‹œì ì¸ íƒœê·¸ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì¼ë‹¨ ì „ì²´ ì¶œë ¥ ìœ ì§€í•˜ë˜ ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€)
                 console.print("[dim blue]ğŸ§  Thinking Process Visualization Enabled (Raw Output)[/dim blue]")
        
        return final_response

    def _setup_cowork_logger(self):
        """Cowork ì „ìš© ë¡œê±° ì„¤ì • (TUI ì—ëŸ¬ ì¶”ì ìš©)"""
        logger = logging.getLogger("cowork")
        logger.setLevel(logging.INFO)
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        if logger.handlers:
            logger.handlers.clear()
            
        fh = logging.FileHandler("cowork.log", encoding="utf-8")
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        logger.addHandler(fh)
        return logger, fh

    def run_cowork_flow(self, user_goal: str, workspace_root: str = ".", use_tui: bool = True) -> str:
        """
        Tiny Cowork v2.0 ì‹¤í–‰ (TUI & Parallel ì§€ì›)
        """
        from tiny_moa.cowork.workspace import WorkspaceContext
        from tiny_moa.cowork.task_queue import TaskQueue, TaskStatus
        from tiny_moa.cowork.planner import PlannerAgent
        from tiny_moa.cowork.skills.file_skills import CoworkFileSkill
        from tiny_moa.ui.dashboard import CoworkDashboard
        from tiny_moa.cowork.parallel_runner import ParallelRunner
        from tiny_moa.cowork.workers.researcher import ResearchWorker
        from tiny_moa.cowork.workers.writer import WriterWorker
        from tiny_moa.cowork.workers.brain_worker import BrainWorker
        from tiny_moa.cowork.workers.tool_worker import ToolWorker
        from rich.live import Live

        # [Fix] Keep reference to handler for cleanup
        logger, log_handler = self._setup_cowork_logger()
        logger.info(f"--- Starting Cowork Session: {user_goal} ---")

        workspace = WorkspaceContext(workspace_root)
        queue = TaskQueue()
        planner = PlannerAgent(self.brain)
        file_skill = CoworkFileSkill(workspace)
        dashboard = CoworkDashboard(user_goal)
        self.dashboard = dashboard

        runner = ParallelRunner(max_workers=4)

        # 0. Pre-process RAG attachments
        # [Fix] Handle @[filename] in Cowork mode
        user_goal, rag_context = self._process_rag_attachments(user_goal, verbose=use_tui)
        if rag_context:
             logger.info(f"RAG Context extracted ({len(rag_context)} chars)")
             if use_tui:
                 dashboard.add_log(f"RAG Context attached from files.", "System")





        # Worker initialization
        researcher = ResearchWorker("Research-1", logger, self)
        writer = WriterWorker("Writer-1", logger, self.brain, file_skill)
        brain_worker = BrainWorker("Brain-1", logger, self.brain)
        tool_worker = ToolWorker("Tool-1", logger, self)

        # 0. Intelligent Routing & Fast Track (Optimization)
        # Check if the task is simple (Tool or Direct) using Brain's router
        route_data = self.brain.route(user_goal)
        route = route_data.get("route", "DIRECT")
        
        # Determine if it's a simple text/file summary request (Heuristic Fast track)
        is_simple_summary = any(kw in user_goal.lower() for kw in ["ìš”ì•½", "ì •ë¦¬", "summarize", "read", "ì½ê³ "]) and len(user_goal) < 50
        
        # [Fix v2] RAG + TOOL ë³µí•© ìš”ì²­ ê°ì§€
        # RAG ì»¨í…ìŠ¤íŠ¸ê°€ ìˆì–´ë„ ë‚ ì”¨/ê²€ìƒ‰/ë‰´ìŠ¤ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ TOOL ë¼ìš°íŒ… ìœ ì§€
        tool_keywords = ["ë‚ ì”¨", "weather", "ê²€ìƒ‰", "search", "ë‰´ìŠ¤", "news", "ì‹œê°„", "time"]
        needs_tool = any(kw in user_goal.lower() for kw in tool_keywords)
        
        if rag_context and not needs_tool:
            logger.info("RAG context detected (no tool needed). Forcing DIRECT/Summary mode.")
            route = "DIRECT"
            is_simple_summary = True
        elif rag_context and needs_tool:
            logger.info("RAG + TOOL hybrid detected. Will execute both.")
            # route ìœ ì§€ (TOOL), is_simple_summaryëŠ” Falseë¡œ

        
        # Decisions
        bypass_llm_planner = (route in ["TOOL", "DIRECT"]) or is_simple_summary
        
        if use_tui:
            live = Live(dashboard.generate_layout(), refresh_per_second=4, screen=False)
            live.start()
            dashboard.add_log("System initialized.", "System")
            if bypass_llm_planner:
                dashboard.add_log(f"Intelligent bypass enabled (Route: {route}).", "System")
            live.update(dashboard.generate_layout())
        
        try:
            # 1. Plan
            context_str = workspace.get_context_description()
            if rag_context:
                context_str += f"\n\n=== Attached File Context ===\n{rag_context}\n============================="



            if use_tui: 
                dashboard.add_log("Analyzing request and creating plan...", "Planner")
                live.update(dashboard.generate_layout())
            
            # [NEW] RAG + TOOL ë³µí•© ìš”ì²­ ì²˜ë¦¬
            if rag_context and needs_tool:
                logger.info("Creating hybrid RAG+TOOL pipeline")
                tasks_data = []
                
                # 1. ë¬¸ì„œ ë¶„ì„ íƒœìŠ¤í¬
                tasks_data.append({
                    "description": f"Analyze the provided file context and summarize: '{user_goal}'",
                    "agent": "brain"
                })
                
                # 2. í•„ìš”í•œ Tool íƒœìŠ¤í¬ ì¶”ê°€
                user_lower = user_goal.lower()
                if any(kw in user_lower for kw in ["ë‚ ì”¨", "weather"]):
                    # ë„ì‹œ ì¶”ì¶œ
                    cities = ["ì„œìš¸", "seoul", "ë„ì¿„", "tokyo", "ë‰´ìš•", "ë¶€ì‚°", "ì¸ì²œ", "ëŒ€êµ¬"]
                    location = "Seoul"
                    for city in cities:
                        if city in user_lower:
                            location = city.title()
                            break
                    tasks_data.append({
                        "description": f"{location} ë‚ ì”¨",
                        "agent": "tool"
                    })
                
                if any(kw in user_lower for kw in ["ë‰´ìŠ¤", "news"]):
                    # Smart Decomposition for News: Split by connection words to isolate news context
                    import re
                    # Split by sentence delimiters. Do NOT split by comma to preserve entity lists (e.g. "A, B, C news")
                    clauses = re.split(r'(?:ê·¸ë¦¬ê³ |and|\.|\?|also)\s+', user_goal)
                    news_clauses = [c for c in clauses if any(kw in c.lower() for kw in ["ë‰´ìŠ¤", "news"])]
                    
                    if not news_clauses:
                        news_clauses = [user_goal] # Fallback
                    
                    for clause in news_clauses:
                        sub_qs = self.brain.decompose_query(clause)
                        for q in sub_qs:
                            tasks_data.append({
                                "description": q,
                                "agent": "tool"
                            })
                
                if any(kw in user_lower for kw in ["ê²€ìƒ‰", "search"]):
                    tasks_data.append({
                        "description": user_goal,  # ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
                        "agent": "tool"
                    })
                
                dashboard.add_log(f"Hybrid plan created with {len(tasks_data)} tasks.", "Planner")
                
            elif route == "TOOL":
                # Decompose complex questions into simple tool tasks
                sub_queries = self.brain.decompose_query(user_goal)
                logger.info(f"Using TOOL decomposition: {sub_queries}")
                tasks_data = [{"description": q, "agent": "tool"} for q in sub_queries]
                if len(sub_queries) > 1:
                    dashboard.add_log(f"Decomposed into {len(sub_queries)} tool tasks.", "Planner")
            elif route == "DIRECT" and not is_simple_summary:
                # Simple direct response, but also check for decomposition
                sub_queries = self.brain.decompose_query(user_goal)
                logger.info(f"Using DIRECT decomposition: {sub_queries}")
                tasks_data = [{"description": q, "agent": "brain"} for q in sub_queries]
            elif is_simple_summary:
                # Heuristic Planning for summary
                logger.info("Using fast-track heuristic plan for summary.")
                
                if rag_context:
                     # If we already have context, we don't need a separate RAG step
                     tasks_data = [
                        {"description": f"Analyze the provided file context and answer the user's question: '{user_goal}'", "agent": "brain"},
                        {"description": "Format the answer clearly", "agent": "writer"}
                     ]
                else:
                    tasks_data = [
                        {"description": f"Locate and read target files related to '{user_goal}'", "agent": "rag"},
                        {"description": "Summarize the extracted content in Korean", "agent": "brain"},
                        {"description": "Save the final summary", "agent": "writer"}
                    ]
            else:
                logger.info("Creating full LLM plan...")
                tasks_data = planner.create_plan(user_goal, context_str)
            
            logger.info(f"Plan created: {tasks_data}")
            
            for t in tasks_data:
                queue.add_task(t.get("description"), t.get("agent", "brain"))
            
            all_tasks = queue.get_all_tasks()
            if use_tui: 
                 dashboard.update_tasks([{"id": t.id, "desc": t.description, "status": t.status.name, "agent": t.agent_type} for t in all_tasks])
                 dashboard.add_log(f"Plan created with {len(tasks_data)} tasks.", "Planner")
                 live.update(dashboard.generate_layout())

            # 2. Execute
            results = []
            
            # [Fix] Inject RAG context into history so workers can see it
            if rag_context:
                results.append(f"[CONTEXT FROM UPLOADED FILES]\n{rag_context}\n[END OF CONTEXT]")
            
            
            # Use ParallelRunner if possible (experimental)
            # Find independent tasks (those with same agent or no clear dependency)
            # For simplicity, we run 'tool' and 'rag' tasks in parallel if multiple.
            # 'brain' and 'writer' usually depend on previous results.
            
            parallelizable = [t for t in all_tasks if t.agent_type in ["tool", "rag"]]
            sequential = [t for t in all_tasks if t.agent_type in ["brain", "writer"]]
            
            # [FIX] Hybrid ëª¨ë“œì—ì„œëŠ” ìˆœì°¨ íƒœìŠ¤í¬(brain)ë¥¼ ë¨¼ì € ì‹¤í–‰
            is_hybrid_mode = rag_context and needs_tool
            if is_hybrid_mode:
                # brainì´ ë¨¼ì €, ê·¸ ë‹¤ìŒ tool
                first_phase = sequential
                second_phase = parallelizable
            else:
                # ê¸°ì¡´: ë³‘ë ¬ ë¨¼ì €, ìˆœì°¨ ë‚˜ì¤‘
                first_phase = parallelizable
                second_phase = sequential
            
            def execute_single_task(task):
                 try:
                    agent_type = task.agent_type.lower()
                    task_lower = task.description.lower()
                    history = "\n\n".join(results) # Note: Parallel tasks won't have latest history from siblings
                    
                    if use_tui:
                        task.status = TaskStatus.RUNNING
                        # Show more detail in log: Agent + Preview
                        dashboard.add_log(f"[{agent_type.upper()}] Thinking: {task.description[:40]}...", agent_type.capitalize())
                        dashboard.update_tasks([{"id": t.id, "desc": t.description, "status": t.status.name, "agent": t.agent_type} for t in all_tasks])
                        live.update(dashboard.generate_layout())

                    if agent_type == "tool":
                        # For tools, we want to know WHICH tool.
                        # We use chat(return_raw_tool_result=True)
                        res = tool_worker.execute(task.description)
                        if use_tui and isinstance(res, dict):
                             # Extract actual info for log
                             t_name = res.get("tool", "unknown")
                             # Search result inner content is in res['result']
                             t_res = res.get("result", {})
                             if isinstance(t_res, dict) and "results" in t_res:
                                  # News/Search result case
                                  articles = t_res["results"]
                                  dashboard.add_log(f"Tool {t_name.upper()}: Found {len(articles)} items", "Tool")
                                  for art in articles:
                                       title = art.get('title', 'No Title')
                                       url = art.get('url') or art.get('href', 'No URL')
                                       dashboard.add_log(f"ARTICLE: {title}", "Source")
                                       dashboard.add_log(f"   URL: {url}", "Source")
                             elif isinstance(t_res, dict) and "temperature" in t_res:
                                  # Weather case
                                  dashboard.add_log(f"Weather: {t_res['temperature']}, {t_res['condition']}", "Tool")
                             else:
                                  dashboard.add_log(f"API {t_name}: {str(t_res)[:50]}...", "Tool")
                             live.update(dashboard.generate_layout())
                        # If raw, we need to extract the 'result' part for the final integration
                        if isinstance(res, dict) and "result" in res:
                             res = res["result"]
                    elif agent_type == "rag":
                        res = researcher.execute(task.description)
                    elif agent_type == "writer":
                        res = writer.execute(task.description, history=history, user_goal=user_goal)
                    else:
                        res = brain_worker.execute(task.description, history=history)
                    
                    # [Critical Fix] Ensure task.result is ALWAYS a string for history/integration
                    task.result = str(res)
                    task.status = TaskStatus.COMPLETED
                    if use_tui:
                        # Log success and a small snippet of the result
                        res_str = str(res)
                        dashboard.add_log(f"Success: {task.id} ({res_str[:50]}...)", agent_type.capitalize())
                        dashboard.update_tasks([{"id": t.id, "desc": t.description, "status": t.status.name, "agent": t.agent_type} for t in all_tasks])
                        live.update(dashboard.generate_layout())
                    return res
                 except Exception as e:
                    task.status = TaskStatus.FAILED
                    task.result = str(e)
                    if use_tui:
                        dashboard.add_log(f"Failed {task.id}: {e}", "Error")
                        dashboard.update_tasks([{"id": t.id, "desc": t.description, "status": t.status.name, "agent": t.agent_type} for t in all_tasks])
                        live.update(dashboard.generate_layout())
                    raise e

            # Run first phase tasks
            if first_phase:
                logger.info(f"Running {len(first_phase)} first-phase tasks.")
                if len(first_phase) > 1 and all(t.agent_type in ["tool", "rag"] for t in first_phase):
                    # Parallel execution for tool/rag
                    t_dicts = [{"id": t.id, "description": t.description, "agent": t.agent_type} for t in first_phase]
                    def runner_wrapper(t_dict):
                        target_task = next(tt for tt in first_phase if tt.id == t_dict['id'])
                        return execute_single_task(target_task)
                    runner.run_tasks(t_dicts, runner_wrapper)
                else:
                    # Sequential execution
                    for t in first_phase: execute_single_task(t)

            # Update results after first phase
            for t in first_phase:
                if t.status == TaskStatus.COMPLETED:
                     results.append(f"[TASK: {t.description}]\nDATA: {t.result}")
            logger.info(f"After first_phase: {len(results)} results collected. Last: {results[-1][:100] if results else 'NONE'}...")

            # Run second phase tasks
            if second_phase:
                logger.info(f"Running {len(second_phase)} second-phase tasks.")
                if len(second_phase) > 1 and all(t.agent_type in ["tool", "rag"] for t in second_phase):
                    # Parallel execution for tool/rag
                    t_dicts = [{"id": t.id, "description": t.description, "agent": t.agent_type} for t in second_phase]
                    def runner_wrapper2(t_dict):
                        target_task = next(tt for tt in second_phase if tt.id == t_dict['id'])
                        return execute_single_task(target_task)
                    runner.run_tasks(t_dicts, runner_wrapper2)
                else:
                    # Sequential execution
                    for t in second_phase: execute_single_task(t)
                    
            # Update results after second phase
            for t in second_phase:
                if t.status == TaskStatus.COMPLETED:
                     results.append(f"[TASK: {t.description}]\nDATA: {t.result}")

            # 3. Final Integration (Synthesis)
            if use_tui: 
                dashboard.add_log("Synthesizing final report...", "Brain")
                live.update(dashboard.generate_layout())
            
            logger.info("Performing final integration...")
            
            # [Optimization] If we have Brain task results (summaries), 
            # we explicitly remove the raw RAG context to prevent it from overwhelming the context window
            # or confusing the model vs the summarized output.
            effective_results = results
            if len(results) > 1 and "[CONTEXT FROM UPLOADED FILES]" in results[0]:
                # Check if we have any valid task outputs (Brain/Tool)
                has_task_output = any("[TASK:" in r for r in results[1:])
                if has_task_output:
                    logger.info("Removing raw RAG context from final integration input as tasks have processed it.")
                    effective_results = results[1:]
            
            input_data = "\n\n".join(effective_results)
            logger.info(f"Input data to Brain: {input_data[:500]}...") # Log first 500 chars to check
            
            with self._model_lock:
                final_report = self.brain.integrate_response(user_goal, input_data)
            
            logger.info(f"Pre-translation output: {final_report}")

            # [English-First Strategy]
            # Brainì€ ì˜ì–´ë¥¼ ìƒì„±í•˜ë¯€ë¡œ, ë§Œì•½ ì‚¬ìš©ì ì§ˆë¬¸ì´ í•œêµ­ì–´ì˜€ë‹¤ë©´(ë˜ëŠ” ë²ˆì—­ íŒŒì´í”„ë¼ì¸ì´ ìˆë‹¤ë©´) í•œêµ­ì–´ë¡œ ë²ˆì—­
            if self.enable_translation and self._translation_pipeline and isinstance(final_report, str):
                if use_tui:
                     dashboard.add_log("Translating report to Korean...", "System")
                     live.update(dashboard.generate_layout())
                
                # íƒ€ê²Ÿ ì–¸ì–´ ê°ì§€ë¥¼ ìœ„í•´ user_goal ì¬ë¶„ì„ (cowork flowëŠ” chatê³¼ ë³„ê°œë¼ ì§ì ‘ ìˆ˜í–‰)
                t_ctx = self._translation_pipeline.to_english(user_goal)
                if t_ctx.is_translated: # user_goalì´ ì˜ì–´ê°€ ì•„ë‹ˆì—ˆë‹¤ë©´ (ì¦‰ í•œêµ­ì–´ ë“±)
                     try:
                         final_report = self._translation_pipeline.from_english(final_report, t_ctx)
                     except Exception as e:
                         logger.error(f"Translation failed (cowork): {e}")

            if use_tui: 
                dashboard.add_log("Flow completed successfully.", "System")
                live.stop()
            
            # Save final report to file (Integrated Result)
            try:
                write_msg = workspace.write_file("docs/cowork_result.md", final_report)
                logger.info(f"Auto-save result: {write_msg}")
                console.print(f"\n[green]â„¹ï¸ ì‘ì—… ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: docs/cowork_result.md[/green]")
            except Exception as e:
                logger.error(f"Failed to auto-save cowork_result.md: {e}")

            logger.info("Cowork flow completed.")
            self.dashboard = None
            return final_report
            
        except Exception as e:
            logger.critical(f"Fatal error in cowork flow: {e}", exc_info=True)
            self.dashboard = None
            if use_tui: live.stop()
            raise e
        finally:
            # [Fix] Clean up log handler
            if log_handler:
                log_handler.close()
                logger.removeHandler(log_handler)


def interactive_mode():
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    console.print(Panel(
        "[bold]ğŸ¤– Tiny MoA ëŒ€í™”í˜• ëª¨ë“œ[/bold]\n"
        "ğŸ”§ Tool Calling: ë‚ ì”¨, ê²€ìƒ‰, ê³„ì‚°, ì‹œê°„\n"
        "ğŸŒ ë‹¤êµ­ì–´ ì§€ì›: í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ë“±\n"
        "ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit'",
        border_style="blue",
    ))
    
    moa = TinyMoA()
    
    while True:
        try:
            user_input = console.input("\n[bold cyan]You:[/bold cyan] ")
            
            if user_input.lower() in ["quit", "exit", "q"]:
                console.print("[dim]ğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”![/dim]")
                break
            
            if not user_input.strip():
                continue
            
            moa.chat(user_input)
            
        except KeyboardInterrupt:
            console.print("\n[dim]ğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”![/dim]")
            break


if __name__ == "__main__":
    interactive_mode()

